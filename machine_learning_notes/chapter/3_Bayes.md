# 贝叶斯算法
# 简介
&#8195;  在所有的机器学习分类算法中，朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数的分类算法，比如决策树、KNN、逻辑回归、支持向量机等，他们都是判别方法，也就是直接学习出特征输出 Y 和特征 X 之间的关系，要么是决策函数 Y=f(X)，要么是条件分布 P(Y|X)。**但是朴素贝叶斯却是生成方法，也就是直接找出特征输出 Y 和特征 X 的联合分布 P(X,Y)，然后用 P(Y|X)=P(X,Y)/P(X) 得出**。

# 一、朴素贝叶斯相关的统计学知识
&#8195;  贝叶斯学派的思想可以概括为**先验概率+数据=后验概率**。也就是说我们在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。一般来说先验概率就是我们对于数据所在领域的历史经验，但是这个经验常常难以量化或者模型化，比如正态分布，beta分布等。是在很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文本分类。

* 我们先看看条件独立公式，如果 X 和 Y 相互独立，则有：
$$P(X,Y) =P(X)P(Y)$$

* 我们接着看看条件概率公式：
$$P(Y|X) = P(X,Y)/P(X) \\
P(X|Y) = P(X,Y)/P(Y)$$

* 或者说:
$$P(Y|X) = P(X|Y)P(Y)/P(X)$$

* 接着看<font color=#0099ff>[ *全概率公式* ](https://zh.wikipedia.org/zh-hans/%E5%85%A8%E6%A6%82%E7%8E%87%E5%85%AC%E5%BC%8F)</font>：
$$P(X) = \sum\limits_{k}P(X|Y =Y_k)P(Y_k)，\; 其中\sum\limits_{k}P(Y_k)=1$$

* 从上面的公式很容易得出贝叶斯公式：
$$P(Y_k|X) = \frac{P(X|Y_k)P(Y_k)}{\sum\limits_{k}P(X|Y =Y_k)P(Y_k)}$$

---

# 二、朴素贝叶斯的模型
* 假如我们的分类模型样本是：
$$(x_1^{(1)}, x_2^{(1)}, ...x_n^{(1)}, y_1), (x_1^{(2)}, x_2^{(2)}, ...x_n^{(2)},y_2), ... (x_1^{(m)}, x_2^{(m)}, ...x_n^{(m)}, y_m)$$

* 即我们有 m 个样本，每个样本有 n 个特征，特征输出有 K 个类别，定义为 $C_1,C_2,...,C_K$。从样本我们可以学习得到朴素贝叶斯的先验分布：
$$P(Y=C_k)，k=1,2,...K$$

* 接着学习到条件概率分布：
$$P(X=x|Y=C_k) = P(X_1=x_1, X_2=x_2,...X_n=x_n|Y=C_k)$$

* 然后我们就可以用贝叶斯公式得到 X 和 Y 的联合分布 P(X,Y) 了。联合分布 P(X,Y) 定义为：
$$\begin{aligned} P(X,Y=C_k)  &= P(Y=C_k)P(X=x|Y=C_k) \\&= P(Y=C_k)P(X_1=x_1, X_2=x_2,...X_n=x_n|Y=C_k) \end{aligned}$$

* 从上面的式子可以看出 $P(Y=C_k)$ 比较容易通过最大似然法求出，得到的 $P(Y=C_k)$ 就是类别 $C_k$ 在训练集里面出现的频数。

* 但是 $P(X_1=x_1,X_2=x_2,...X_n=x_n|Y=C_k)$ 很难求出，这是一个有 n 个维度的条件分布。朴素贝叶斯模型在这里做了一个大胆的假设：**即 X 的 n 个维度之间相互独立**，这样就可以得出:
$$P(X_1=x_1, X_2=x_2,...X_n=x_n|Y=C_k) \\
 = P(X_1=x_1|Y=C_k)P(X_2=x_2|Y=C_k)...P(X_n=x_n|Y=C_k)$$

* 从上式可以看出，这个很难的条件分布大大的简化了，但是这也**可能带来预测的不准确性**。**如果特征之间非常不独立的话，那就尽量不要使用朴素贝叶斯模型了，考虑使用其他的分类方法比较好。但是一般情况下，样本的特征之间独立这个条件的确是弱成立的，尤其是数据量非常大的时候。虽然我们牺牲了准确性，但是得到的好处是模型的条件分布的计算大大简化了，这就是贝叶斯模型的选择**。

* 最后回到我们要解决的问题，我们的问题是给定测试集的一个新样本特征 $(x_1^{(test)}, x_2^{(test)}, ...x_n^{(test)})$，我们如何判断它属于哪个类型？

    * 既然是贝叶斯模型，当然是**后验概率最大化来判断分类**。我们只要**计算出所有的 K 个条件概率 $P(Y=C_k|X=X^{(test)})$，然后找出最大的条件概率对应的类别，这就是朴素贝叶斯的预测**。

---

# 三、朴素贝叶斯的推断过程
* 我们预测的类别 $C_{result}$ 是使 $P(Y=C_k|X=X^{(test)})$ 最大化的类别，数学表达式为：
$$\begin{aligned}C_{result}  & = \underbrace{argmax}_{C_k}P(Y=C_k|X=X^{(test)})\\
& = \underbrace{argmax}_{C_k}P(X=X^{(test)}|Y=C_k)P(Y=C_k) \Bigg/P(X=X^{(test)})\end{aligned}$$

* 由于对于所有的类别计算 $P(Y=C_k|X=X^{(test)})$ 时，上式的分母是一样的，都是 $P(X=X^{(test)}$，因此，我们的预测公式可以简化为：
$$C_{result}  = \underbrace{argmax}_{C_k}P(X=X^{(test)}|Y=C_k)P(Y=C_k)$$

* 接着我们利用朴素贝叶斯的独立性假设，就可以得到通常意义上的朴素贝叶斯推断公式:
$$C_{result}  = \underbrace{argmax}_{C_k}P(Y=C_k)\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)$$

---

# 四、朴素贝叶斯的参数估计
* 对于 $P(Y=C_k)$，比较简单，通过极大似然估计我们很容易得到 $P(Y=C_k)$ 为样本类别 $C_k$ 出现的频率，即样本类别 $C_k$ 出现的次数 $m_k$ 除以样本总数 m。

* 对于 $P(X_j=X^{(test)}_j|Y=C_k)(j=1,2,...n)$，这个取决于我们的先验条件：

  * 1. **如果我们的 $X_j$ 是离散的值，那么我们可以假设 $X_j$ 符合多项式分布**，这样得到 $P(X_j=X^{(test)}_j|Y=C_k)$ 是在样本类别 $C_k$ 中，特征 $X^{(test)}_j$ 出现的频率。即：
$$P(X_j=X_j^{(test)}|Y=C_k) = \frac{m_{kj^{test}}}{m_k}$$
其中 $m_k$ 为样本类别 $C_k$ 总的特征计数，而 $m_{kj^{test}}$ 为类别为 $C_k$ 的样本中，第 j 维特征 $X^{(test)}_j$ 出现的计数。某些时候，可能某些类别在样本中没有出现，这样可能导致 $P(X_j=X^{(test)}_j|Y=C_k)$ 为0，这样会影响后验的估计，为了解决这种情况，我们引入了拉普拉斯平滑，即此时有：
$$P(X_j=X_j^{(test)}|Y=C_k) = \frac{m_{kj^{test}} + \lambda}{m_k + O_j\lambda}$$
其中 λ 为一个大于0的常数，常常取为1，$O_j$ 为第 j 个特征的取值个数。

  * 2. **如果我们我们的 $X_j$ 是非常稀疏的离散值，即各个特征出现概率很低，这时我们可以假设 $X_j$ 符合伯努利分布，即特征 $X_j$ 出现记为1，不出现记为0**。即只要 $X_j$ 出现即可，我们不关注 $X_j$ 的次数。这样得到 $P(X_j=X^{(test)}_j|Y=C_k)$ 是在样本类别 $C_k$ 中，$X^{(test)}_j$ 出现的频率。此时有：
$$P(X_j=X_j^{(test)}|Y=C_k) \\
= P(X_j=1|Y=C_k)X_j^{(test)} + (1 - P(X_j=1|Y=C_k))(1-X_j^{(test)})$$
其中，$X^{(test)}_j$ 取值为0和1。

  * 3. **如果我们我们的 $X_j$ 是连续值，我们通常取 $X_j$ 的先验概率为正态分布，即在样本类别 $C_k$ 中，$X_j$ 的值符合正态分布**。这样 $P(X_j=X^{(test)}_j|Y=C_k)$ 的概率分布是：
$$P(X_j=X_j^{(test)}|Y=C_k) = \frac{1}{\sqrt{2\pi\sigma_k^2}}exp\Bigg( -\frac{(X_j^{(test)} - \mu_k)^2}{2\sigma_k^2}\Bigg)$$
其中 $μ_k$ 和 $σ^{2}_k$ 是正态分布的期望和方差，可以通过极大似然估计求得。$μ_k$ 为在样本类别 $C_k$ 中，所有 $X_j$ 的平均值。$σ^{2}_k$ 为在样本类别 $C_k$ 中，所有 $X_j$ 的方差。对于一个连续的样本值，带入正态分布的公式，就可以求出概率分布了。

---

# 五、朴素贝叶斯算法过程
* 我们假设训练集为 m 个样本 n 个维度，如下：
$$(x_1^{(1)}, x_2^{(1)}, ...x_n^{(1)}, y_1), (x_1^{(2)}, x_2^{(2)}, ...x_n^{(2)},y_2), ... (x_1^{(m)}, x_2^{(m)}, ...x_n^{(m)}, y_m)$$

* 共有 K 个特征输出类别，分别为 $C_1,C_2,...,C_K$，每个特征输出类别的样本个数为 $m_1,m_2,...,m_K$，在第 k 个类别中，如果是离散特征，则特征 $X_j$ 各个类别取值为 $m_{kji}$。其中 i 取值为 $1,2,...S_j$，$S_j$ 为特征 j 不同的取值数，输出为实例 $X^{(test)}$ 的分类。

* 算法流程如下：

    * 如果没有 Y 的先验概率，则计算 Y 的 K 个先验概率：$P(Y=C_k)=(m_k+λ)/(m+Kλ)$，否则 $P(Y=C_k)$ 为输入的先验概率。

    * 分别计算第 k 个类别的第 j 维特征的第 i 个取值条件概率：$P(X_j=x_{ji}|Y=C_k)$
        * a. 如果是离散值，λ 可以取值为1，或者其他大于0的数字；
$$P(X_j=x_{ji}|Y=C_k) = \frac{m_{kji} + \lambda}{m_k + S_j\lambda}$$

        * b. 如果是稀疏二项离散值，此时 i 只有两种取值；
$$P(X_j=x_{ji}|Y=C_k) = P(j|Y=C_k)x_{ji} + (1 - P(j|Y=C_k)(1-x_{ji})$$

        * c. 如果是连续值不需要计算各个 i 的取值概率，直接求正态分布的参数。需要求出 $μ_k$ 和 $σ^{2}_k$。$μ_k$ 为在样本类别 $C_k$ 中，所有 $X_j$ 的平均值。$σ^{2}_k$ 为在样本类别 $C_k$ 中，所有 $X_j$ 的方差。

    * 3. 对于实例 $X^{(test)}$，分别计算：
$$P(Y=C_k)\prod_{j=1}^{n}P(X_j=x_j^{(test)}|Y=C_k)$$

    * 4. 确定实例 $X^{(test)}$ 的分类 $C_{result}$：
$$C_{result}  = \underbrace{argmax}_{C_k}P(Y=C_k)\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)$$

从上面的计算可以看出，没有复杂的求导和矩阵运算，因此效率很高。

---

# 六、朴素贝叶斯算法小结
* 主要优点

    * 朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率；
    
    * 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去增量训练；
    
    * 对缺失数据不太敏感，算法也比较简单，常用于文本分类。

* 主要缺点

    * 在属性个数比较多或者属性之间相关性较大时，分类效果不好；
    
    * 需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳；
    
    * 由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率；
    
    * 对输入数据的表达形式很敏感。

---

# 七、题目
## 1. 朴素贝叶斯与LR的区别？
* 朴素贝叶斯是生成模型，根据已有样本进行贝叶斯估计学习出先验概率 P(Y) 和条件概率 P(X|Y)，进而求出联合分布概率 P(XY)，最后利用贝叶斯定理求解 P(Y|X)；而LR是判别模型，根据极大化对数似然函数直接求出条件概率 P(Y|X)。

* 朴素贝叶斯是基于很强的条件独立假设（在已知分类Y的条件下，各个特征变量取值是相互独立的），而 LR 则对此没有要求。

* 朴素贝叶斯适用于数据集少的情景，而 LR 适用于大规模数据集。

## 2. 朴素贝叶斯“朴素”在哪里？
* 简单来说：利用贝叶斯定理求解联合概率 P(XY) 时，需要计算条件概率 P(X|Y)。在计算 P(X|Y) 时，朴素贝叶斯做了一个很强的条件独立假设（当 Y 确定时，X 的各个分量取值之间相互独立），即 $P(X1=x_1,X2=x_2,...Xj=x_j|Y=y_k) = P(X1=x_1|Y=y_k) P(X2=x_2|Y=y_k) ... P(X_j=x_j|Y=y_k)$。

## 3. 在估计条件概率 P(X|Y) 时出现概率为0的情况怎么办？
* 零概率问题，就是在计算实例的概率时，如果某个量 x，在观察样本库（训练集）中没有出现过，会导致整个实例的概率结果是0。在文本分类的问题中，当一个词语没有在训练样本中出现，该词语调概率为0，使用连乘计算文本出现概率时也为0。这是不合理的，不能因为一个事件没有观察到就武断的认为该事件的概率是0。

* 为了解决零概率的问题，法国数学家拉普拉斯最早提出用加1的方法估计没有出现过的现象的概率，所以加法平滑也叫做拉普拉斯平滑。 假定训练样本很大时，每个分量 x 的计数加1造成的估计概率变化可以忽略不计，但可以方便有效的避免零概率问题。简单来说：引入 λ，当 λ=1 时称为拉普拉斯平滑。

## 4. 朴素贝叶斯的优缺点
* 优点：对小规模的数据表现很好，适合多分类任务，适合增量式训练。

* 缺点：对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。

## 5. 为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果?
* 对于分类任务来说，只要各类别的条件概率排序正确、无需精准概率值即可导致正确分类；

* 如果属性间依赖对所有类别影响相同，或依赖关系的影响能相互抵消，则属性条件独立性假设在降低计算开销的同时不会对性能产生负面影响。

## 6. 算法问题
* 实际项目中，概率值往往是很小的小数，连续微小小数相乘容易造成下溢出使乘积为0.

* 解决方法：对乘积取自然对数，将连乘变为连加。

* 另外需要注意：给出的特征向量长度可能不同，这是需要归一化为通长度的向量（这里以文本分类为例），比如说是句子单词的话，则长度为整个词汇量的长度，对应位置是该单词出现的次数。

## 7. 先验条件概率的计算方法：
* 离散分布时：统计训练样本中每个类别出现的频率。若某一特征值的概率为0会使整个概率乘积变为0（称为数据稀疏），这破坏了各特征值地位相同的假设条件。

    * 解决方法一：采用贝叶斯估计（λ=1 时称为拉普拉斯平滑）：

    * 解决方法二：通过聚类将未出现的词找出系统关键词，根据相关词的概率求平均值。

* 连续分布时：假定其值服从高斯分布（正态分布）。即计算样本均值与方差。



