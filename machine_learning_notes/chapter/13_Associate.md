# 关联算法
# 一、Apriori关联算法
## 简介

&#8195;  **Apriori 算法**是常用的用于**挖掘出数据关联规则**的算法，它用来**找出数据值中频繁出现的数据集合**，找出这些集合的模式有助于我们做一些决策。比如在常见的超市购物数据集，或者电商的网购数据集中，如果我们找到了频繁出现的数据集，那么对于超市，我们可以优化产品的位置摆放，对于电商，我们可以优化商品所在的仓库位置，达到节约成本，增加经济效益的目的。下面我们就对 Apriori 算法做一个总结。

## 1. 频繁项集的评估标准

&#8195; 项集：最基本的模式是项集，它是指若干个项的集合。频繁模式是指数据集中频繁出现的项集、序列或子结构。频繁项集是指支持度大于等于最小支持度($min_{sup}$)的集合。其中支持度是指某个集合在所有事务中出现的频率。什么样的数据才是频繁项集呢？肉眼一扫，一起**出现次数多的数据集就是频繁项集**，但是有两个问题：

* 第一是当数据量非常大的时候，我们没法直接肉眼发现频繁项集，这催生了关联规则挖掘的算法，比如 Apriori, PrefixSpan，CBA。

* 第二是我们缺乏一个频繁项集的标准。比如10条记录，里面 A 和 B 同时出现了三次，那么我们能不能说 A 和 B 一起构成频繁项集呢？因此我们需要一个评估频繁项集的标准。

常用的频繁项集的评估标准有**支持度、置信度**和**提升度**三个。

### 1.1 支持度
* **支持度**就是几个关联的数据在数据集中出现的次数占总数据集的比重。或者说几个数据关联出现的概率。如果我们有两个想分析关联性的数据 $X 和 Y$，则对应的支持度为:
$$Support(X,Y) = P(XY) = \frac{number(XY)}{num(All Samples)}$$
以此类推，如果我们有三个想分析关联性的数据 $X，Y 和 Z$，则对应的支持度为：
$$Support(X,Y,Z) = P(XYZ) = \frac{number(XYZ)}{num(All Samples)}$$
一般来说，支持度高的数据不一定构成频繁项集，但是支持度太低的数据肯定不构成频繁项集。

### 1.2 置信度
* **置信度**体现了一个数据出现后，另一个数据出现的概率，或者说数据的条件概率。如果我们有两个想分析关联性的数据 $X$ 和 $Y，X$ 对 $Y$ 的置信度为：
$$Confidence(X \Leftarrow Y) = P(X|Y)=P(XY)/P(Y)$$
以此类推到多个数据的关联置信度，比如对于三个数据 $X，Y，Z$，则 $X$ 对于 $Y$ 和 $Z$ 的置信度为：
$$Confidence(X \Leftarrow YZ) = P(X|YZ)=P(XYZ)/P(YZ)$$
举个例子，在购物数据中，纸巾对应鸡爪的置信度为 40%，支持度为 1%。则意味着在购物数据中，总共有1%的用户既买鸡爪又买纸巾；同时买鸡爪的用户中有 40% 的用户购买纸巾。

### 1.3 提升度
* **提升度**表示含有 $Y$ 的条件下，同时含有 $X$ 的概率，与 $X$ 总体发生的概率之比，即:
$$Lift(X \Leftarrow Y) = P(X|Y)/P(X) = Confidence(X \Leftarrow Y) / P(X)$$
提升度体先了 $X$ 和 $Y$ 之间的关联关系, 提升度大于1则 $X⇐Y$ 是有效的强关联规则， 提升度小于等于1则 $X⇐Y$ 是无效的强关联规则 。一个特殊的情况，如果 $X$ 和 $Y$ 独立，则有 $Lift(X⇐Y)=1$，因为此时 $P(X|Y)=P(X)$。

&#8195; 一般来说，要选择一个数据集合中的频繁数据集，则需要自定义评估标准。最常用的评估标准是用自定义的支持度，或者是自定义支持度和置信度的一个组合。

## 2. 算法思想

&#8195; 对于Apriori算法，我们使用**支持度**来作为我们判断频繁项集的标准。Apriori算法的目标是找到最大的 K 项频繁集。这里有两层意思：首先，我们要找到符合支持度标准的频繁集，但是这样的频繁集可能有很多；第二层意思就是我们要找到最大个数的频繁集。比如我们找到符合支持度的频繁集 AB 和 ABE，那么我们会抛弃 AB，只保留 ABE，因为 AB 是2项频繁集，而 ABE 是3项频繁集。

&#8195; Apriori算法采用了**迭代**的方法，先搜索出候选1项集及对应的支持度，剪枝去掉低于支持度的1项集，得到频繁1项集。然后对剩下的频繁1项集进行连接，得到候选的频繁2项集，筛选去掉低于支持度的候选频繁2项集，得到真正的频繁二项集，以此类推，迭代下去，直到无法找到频繁 k+1 项集为止，对应的频繁 k 项集的集合即为算法的输出结果。

&#8195; 可见这个算法还是很简洁的，第 i 次的迭代过程包括扫描计算候选频繁 i 项集的支持度，剪枝得到真正频繁 i 项集和连接生成候选频繁 i+1 项集三步。我们拿下面这个简单的例子看看：

![](https://upload-images.jianshu.io/upload_images/16911112-80caff6d7d680061.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

&#8195; 我们的数据集 D 有4条记录，分别是 {1,3,4}、{2,3,5}、{1,2,3,5} 和 {2,5}。现在我们用 Apriori算法来寻找频繁 k 项集，最小支持度设置为50%。

* 首先我们生成候选频繁1项集，包括我们所有的5个数据并计算5个数据的支持度，计算完毕后我们进行剪枝，数据4由于支持度只有25%被剪掉。我们最终的频繁1项集为 {1}、{2}、{3} 和 {5}，现在我们链接生成候选频繁2项集，包括 {1,2}、{1,3}、{1,5}、{2,3}、{2,5}、{3,5} 共6组。此时我们的第一轮迭代结束。

* 进入第二轮迭代，我们扫描数据集计算候选频繁2项集的支持度，接着进行剪枝，由于 {1,2} 和 {1,5} 的支持度只有25%而被筛除，得到真正的频繁2项集，包括 {1,3}、{2,3}、{2,5}、{3,5}。现在我们链接生成候选频繁3项集 {1,2,3}、{1,2,5}、{1,3,5} 和 {2,3,5} 共4组，这部分图中没有画出。通过计算候选频繁3项集的支持度，我们发现 {1,2,3}、{1,2,5} 和 {1,3,5} 的支持度均为25%，因此接着被剪枝，最终得到的真正频繁3项集为 {2,3,5} 一组。由于此时我们无法再进行数据连接，进而得到候选频繁4项集，最终的结果即为频繁3三项集 {2,3,5}。


## 3. 算法流程
* 输入：数据集合D，支持度阈值 α
* 输出：最大的频繁 k 项集
### 算法流程
1. 扫描整个数据集，得到所有出现过的数据，作为候选频繁1项集。k=1，频繁0项集为空集。

2. 挖掘频繁k项集
  * a. 扫描数据计算候选频繁 k 项集的支持度；
  
  * b. 去除候选频繁 k 项集中支持度低于阈值的数据集,得到频繁k项集。如果得到的频繁 k 项集为空，则直接返回频繁 k-1 项集的集合作为算法结果，算法结束。如果得到的频繁k项集只有一项，则直接返回频繁 k 项集的集合作为算法结果，算法结束；
  * c. 基于频繁 k 项集，连接生成候选频繁 k+1 项集。

3. 令 k=k+1，转入步骤2。

从算法的步骤可以看出，Aprior算法每轮迭代都要扫描数据集，因此在数据集很大，数据种类很多的时候，算法效率很低。

## 4. 总结

&#8195;  Aprior算法是一个非常经典的频繁项集的挖掘算法，很多算法都是基于 Aprior算法而产生的，包括 FP-Tree，GSP，CBA 等。这些算法利用了 Aprior算法的思想，但是对算法做了改进，数据挖掘效率更好一些，因此现在一般很少直接用 Aprior算法来挖掘数据了，但是理解 Aprior算法是理解其它 Aprior类算法的前提，同时算法本身也不复杂，因此值得好好研究一番。

---

# 二、FP Tree算法

&#8195;  作为一个挖掘频繁项集的算法，Apriori算法需要多次扫描数据，I/O 是很大的瓶颈。为了解决这个问题，**FP Tree** 算法（也称**FP Growth算法**）采用了一些技巧，无论多少数据，只需要扫描两次数据集，因此提高了算法运行的效率。

## 1. FP Tree数据结构

&#8195;  为了减少 I/O 次数，FP Tree算法引入了一些数据结构来临时存储数据。这个数据结构包括三部分，如下图所示：

![](https://upload-images.jianshu.io/upload_images/16911112-8b2d79d6ef50d588.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

* 第一部分是一个项头表。里面记录了所有的1项频繁集出现的次数，按照次数降序排列。比如上图中 B 在所有10组数据中出现了8次，因此排在第一位，这部分好理解。

* 第二部分是 FP Tree，它将我们的原始数据集映射到了内存中的一颗 FP树。

* 第三部分是节点链表。所有项头表里的1项频繁集都是一个节点链表的头，它依次指向 FP树中该1项频繁集出现的位置。这样做主要是方便项头表和 FP Tree 之间的联系查找和更新，也好理解。

## 2. 项头表的建立

&#8195;  FP树的建立需要首先依赖**项头表的建立**。首先我们看看怎么建立项头表。

&#8195;  **我们第一次扫描数据，得到所有频繁一项集的的计数**。然后删除支持度低于阈值的项，将1项频繁集放入项头表，并按照支持度降序排列。接着第二次也是最后一次扫描数据，将读到的原始数据剔除非频繁1项集，并按照支持度降序排列，我们用下面这个例子来具体讲解。

* 我们有10条数据，首先第一次扫描数据并对1项集计数，我们发现 {O I L J P M N} 都只出现一次，支持度低于20%的阈值，因此他们不会出现在下面的项头表中。剩下的 {A C E G B D F} 按照支持度的大小降序排列，组成了我们的项头表。

* 接着我们第二次扫描数据，对于每条数据剔除非频繁1项集，并按照支持度降序排列。比如数据项 {A B C E F O} ，里面 O 是非频繁1项集，因此被剔除，只剩下了 {A B C E F}。按照支持度的顺序排序，它变成了 {A C E B F}。其他的数据项以此类推，这样做是为了我们后面的 FP树的建立时，可以尽可能的共用祖先节点。

* 通过两次扫描，项头表已经建立，排序后的数据集也已经得到了，如下图所示。

![](https://upload-images.jianshu.io/upload_images/16911112-117920a53f147329.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 3. FP Tree的建立

&#8195;  有了项头表和排序后的数据集，我们就可以开始 FP树的建立了。开始时 FP树没有数据，建立 FP树时我们一条条的读入排序后的数据集，插入 FP树，插入时按照排序后的顺序，插入 FP树中，排序靠前的节点是祖先节点，而靠后的是子孙节点。如果有共用的祖先，则对应的公用祖先节点计数加1。插入后，如果有新节点出现，则项头表对应的节点会通过节点链表链接上新节点。直到所有的数据都插入到 FP树后，FP树的建立完成，我们用前面的例子来描述。

* 首先，我们插入第一条数据 {A C E B F}，如下图所示。此时 {F P} 树没有节点，因此 {A C E B F} 是一个独立的路径，所有节点计数为1，项头表通过节点链表链接上对应的新增节点。

![](https://upload-images.jianshu.io/upload_images/16911112-dacbf23388d82cef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

* 接着我们插入数据 {A C G}，如下图所示。由于 {A C G} 和现有的 FP树可以有共有的祖先节点序列 {A C}，因此只需要增加一个新节点 G，将新节点 G 的计数记为1。同时 A 和 C 的计数加1成为2。当然，对应的 G 节点的节点链表要更新。

![](https://upload-images.jianshu.io/upload_images/16911112-1b7d94e8963ef2e3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

* 用同样的办法更新后面8条数据，如下8张图所示。

![](https://upload-images.jianshu.io/upload_images/16911112-a7cfd25f81e9204c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](https://upload-images.jianshu.io/upload_images/16911112-6a964f611542ada9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](https://upload-images.jianshu.io/upload_images/16911112-46bc5d3558df0365.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](https://upload-images.jianshu.io/upload_images/16911112-a6f7a3df0dd9ec4a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](https://upload-images.jianshu.io/upload_images/16911112-c28c905e4a54a8df.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](https://upload-images.jianshu.io/upload_images/16911112-72f541629e086c8f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](https://upload-images.jianshu.io/upload_images/16911112-191e3502ca005d88.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](https://upload-images.jianshu.io/upload_images/16911112-b7710b9c63e0d585.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

---

## 4. FP Tree的挖掘

&#8195;  得到了 FP树和项头表以及节点链表，我们首先要**从项头表的底部项依次向上挖掘**。对于项头表对应于FP树的每一项，我们要找到它的**条件模式基**。所谓条件模式基是以**我们要挖掘的节点作为叶子节点所对应的FP子树**。得到这个FP子树，我们将子树中每个节点的的计数设置为叶子节点的计数，并删除计数低于支持度的节点。从这个条件模式基，我们就可以递归挖掘得到频繁项集了。

* 我们看看先从最底下的 F 节点开始，我们先来寻找 F 节点的条件模式基，由于 F 在 FP树中只有一个节点，因此候选就只有下图左所示的一条路径，对应 {A:8, C:8, E:6, B:2, F:2}。我们接着将所有的祖先节点计数设置为叶子节点的计数，即 FP子树变成 {A:2, C:2, E:2, B:2, F:2}。一般我们的条件模式基可以不写叶子节点，因此最终的 F 的条件模式基如下图右所示。

![](https://upload-images.jianshu.io/upload_images/16911112-c060977b34a65e86.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

* 通过它，我们很容易得到F的频繁2项集为 {A:2, F:2}，{C:2, F:2}，{E:2, F:2}，{B:2, F:2}。递归合并二项集，得到频繁三项集为 {A:2, C:2, F:2}，{A:2, E:2, F:2}，...还有一些频繁三项集，就不写了。当然一直递归下去，最大的频繁项集为频繁5项集，为 {A:2, C:2, E:2, B:2, F:2}。

* F 挖掘完了，我们开始挖掘D节点。D 节点比 F 节点复杂一些，因为它有两个叶子节点，因此首先得到的 FP子树如下图左。我们接着将所有的祖先节点计数设置为叶子节点的计数，即变成 {A:2, C:2,E:1 G:1, D:1, D:1} 此时 E 节点和 G 节点由于在条件模式基里面的支持度低于阈值，被我们删除，最终在去除低支持度节点并不包括叶子节点后 D 的条件模式基为 {A:2, C:2}。通过它，我们很容易得到 D 的频繁2项集为 {A:2, D:2}, {C:2, D:2}。递归合并二项集，得到频繁三项集为 {A:2, C:2, D:2}。D 对应的最大的频繁项集为频繁3项集。

![](https://upload-images.jianshu.io/upload_images/16911112-ce24e25a166b7f41.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

* 同样的方法可以得到 B 的条件模式基如下图右边，递归挖掘到 B 的最大频繁项集为频繁4项集 {A:2, C:2, E:2, B:2}。

![](https://upload-images.jianshu.io/upload_images/16911112-94850750a3265cfd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

* 继续挖掘 G 的频繁项集，挖掘到的 G 的条件模式基如下图右边，递归挖掘到 G 的最大频繁项集为频繁4项集 {A:5, C:5, E:4, G:4}。

![](https://upload-images.jianshu.io/upload_images/16911112-cb230e7c036d03c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

* E 的条件模式基如下图右边，递归挖掘到E的最大频繁项集为频繁3项集 {A:6, C:6, E:6}。

![](https://upload-images.jianshu.io/upload_images/16911112-5c0027c7170013e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

* C的条件模式基如下图右边，递归挖掘到 C 的最大频繁项集为频繁2项集{A:8, C:8}。

![](https://upload-images.jianshu.io/upload_images/16911112-e900b61d70c645ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

* 至于 A，由于它的条件模式基为空，因此可以不用去挖掘了。

&#8195;  至此我们得到了所有的频繁项集，如果我们只是要最大的频繁 K 项集，从上面的分析可以看到，最大的频繁项集为5项集。包括 {A:2, C:2, E:2, B:2, F:2}。

## 5. FP Tree算法归纳

* 1. 扫描数据，得到所有频繁一项集的的计数。然后删除支持度低于阈值的项，将1项频繁集放入项头表，并按照支持度降序排列。

* 2. 扫描数据，将读到的原始数据剔除非频繁1项集，并按照支持度降序排列。

* 3. 读入排序后的数据集，插入FP树，插入时按照排序后的顺序，插入FP树中，排序靠前的节点是祖先节点，而靠后的是子孙节点。如果有共用的祖先，则对应的公用祖先节点计数加1。插入后，如果有新节点出现，则项头表对应的节点会通过节点链表链接上新节点。直到所有的数据都插入到 FP树后，FP树的建立完成。

* 4. 从项头表的底部项依次向上找到项头表项对应的条件模式基。从条件模式基递归挖掘得到项头表项项的频繁项集。

* 5. 如果不限制频繁项集的项数，则返回步骤4所有的频繁项集，否则只返回满足项数要求的频繁项集。

## 6. FP tree算法总结

&#8195;  FP Tree算法改进了 Apriori算法的 I/O 瓶颈，巧妙的利用了树结构，利用内存数据结构以空间换时间是常用的提高算法运行时间瓶颈的办法。在实践中，FP Tree算法是可以用于生产环境的关联算法，而Apriori算法则做为先驱，起着关联算法指明灯的作用。

---

# 三、PrefixSpan算法

&#8195;  前面我们讲到频繁项集挖掘的关联算法 Apriori 和 FP Tree，这两个算法都是挖掘频繁项集的。PrefixSpan算法也是关联算法，但是它是挖掘频繁序列模式的，因此要解决的问题目标稍有不同。

## 1. 项集数据和序列数据

&#8195;  首先我们看看项集数据和序列数据有什么不同，如下图所示。

![](https://upload-images.jianshu.io/upload_images/16911112-9434ef810dfcd010.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

&#8195;  左边的数据集就是项集数据，每个项集数据由若干项组成，这些项没有时间上的先后关系。而右边的序列数据则不一样，它是由若干数据项集组成的序列。比如第一个序列 <a(abc)(ac)d(cf)> ，它由 a,abc,ac,d,cf 共5个项集数据组成，并且这些项**有时间上的先后关系**。**对于多于一个项的项集我们要加上括号，以便和其他的项集分开**。同时由于项集内部是不区分先后顺序的，为了方便数据处理，我们一般**将序列数据内所有的项集内部按字母顺序排序**。

## 2. 子序列与频繁序列

&#8195;  子序列和我们数学上的子集的概念很类似，也就是说，如果某个序列 A 所有的项集在序列 B 中的项集都可以找到，则 A 就是 B 的子序列。当然，如果用严格的数学描述，子序列是这样的：

&#8195;  对于序列 $A={a_1, a_2, ... a_n}$ 和序列 $B={b_1,b_2,...b_m}, n≤m$，如果存在数字序列 $1≤j_1≤j_2≤...≤j_n≤m$，满足 $a_1⊆b_{j_1},a_2⊆b_{j_2},... a_n⊆b_{j_n}$，则称 A 是 B 的子序列。当然反过来说，B 就是 A 的超序列。

&#8195;  而频繁序列则和我们的频繁项集很类似，也就是频繁出现的子序列。比如对于下图，支持度阈值定义为50%，也就是需要出现两次的子序列才是频繁序列。而子序列 <(ab)c> 是频繁序列，因为它是图中的第一条数据和第三条序列数据的子序列，对应的位置用蓝色标示。

![](https://upload-images.jianshu.io/upload_images/16911112-7d858221fe163081.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 3. 算法的一些概念

&#8195; **PrefixSpan算法**的全称是 **Prefix-Projected Pattern Growth**，即**前缀投影的模式挖掘**。在 PrefixSpan算法中的前缀 Prefix 通俗意义讲就是序列数据前面部分的子序列，比如对于序列数据 B = <a(abc)(ac)d(cf)>，而 A= <a(abc)a>，则 A 是 B 的前缀。当然 B 的前缀不止一个，比如 \<a>,\<aa>,\<a(ab)> 也都是 B 的前缀。

&#8195;  看了前缀，我们再来看前缀投影，其实前缀投影这儿就是我们的后缀，有前缀就有后缀嘛。前缀加上后缀就可以构成一个我们的序列。下面给出前缀和后缀的例子。对于某一个前缀，序列里前缀后面剩下的子序列即为我们的后缀。如果前缀最后的项是项集的一部分，则用一个“_”来占位表示。

&#8195;  下面这个例子展示了序列 <a(abc)(ac)d(cf)> 的一些前缀和后缀，还是比较直观的。要注意的是，**如果前缀的末尾不是一个完全的项集，则需要加一个占位符**。在 PrefixSpan算法中，相同前缀对应的所有后缀的结合我们称为前缀对应的投影数据库。

![](https://upload-images.jianshu.io/upload_images/16911112-987824f54788b48d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 4. 算法思想

&#8195;  PrefixSpan算法的目标是挖掘出满足最小支持度的频繁序列。回忆 Aprior算法，它是从频繁1项集出发，一步步的挖掘2项集，直到最大的 K 项集。PrefixSpan算法也类似，它从长度为1的前缀开始挖掘序列模式，搜索对应的投影数据库得到长度为1的前缀对应的频繁序列，然后递归的挖掘长度为2的前缀所对应的频繁序列。以此类推，一直递归到不能挖掘到更长的前缀挖掘为止。

&#8195;  里面长度为1的前缀包括 \<a>,\<b>,\<c>,\<d>,\<e>,\<f>,\<g> 我们需要对这6个前缀分别递归搜索找各个前缀对应的频繁序列。如下图所示，每个前缀对应的后缀也标出来了。由于 g 只在序列4出现，支持度计数只有1，因此无法继续挖掘。我们的长度为1的频繁序列为 \<a>,\<b>,\<c>,\<d>,\<e>,\<f>。去除所有序列中的 g，即第4条记录变成 <e(af)cbc>。

![](https://upload-images.jianshu.io/upload_images/16911112-938b135b03128a56.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

&#8195;  现在我们开始挖掘频繁序列,分别从长度为1的频繁项开始。这里我们以d为例子来递归挖掘，其他的节点递归挖掘方法和 D 一样，方法如下图。

* 首先我们对ｄ的后缀进行计数，得到 {a:1, b:2, c:3, d:0, e:1, f:1，_f:1}。注意 f 和 _f 是不一样的，因为前者是在和前缀 d 不同的项集，而后者是和前缀 d 同项集。由于此时 a,d,e,f,_f 都达不到支持度阈值，因此我们递归得到的前缀为 d 的2项频繁序列为 \<db> 和 \<dc>。

* 接着我们分别递归 db 和 dc 为前缀所对应的投影序列。首先看 db 前缀，此时对应的投影后缀只有 <_c(ae)>，此时 _c,a,e 支持度均达不到阈值，因此无法找到以 db 为前缀的频繁序列。

* 现在我们来递归另外一个前缀dc。以dc为前缀的投影序列为 \<_f>,\<(bc)(ae)>,\<b>，此时我们进行支持度计数，结果为 {b:2, a:1, c:1, e:1, _f:1}，只有 b 满足支持度阈值，因此我们得到前缀为 dc 的三项频繁序列为 <dcb>。

* 我们继续递归以 <dcb> 为前缀的频繁序列。由于前缀 <dcb> 对应的投影序列 <(_c)ae> 支持度全部不达标，因此不能产生4项频繁序列。至此以 d 为前缀的频繁序列挖掘结束，产生的频繁序列为 \<d>\<db>\<dc>\<dcb>。

&#8195;  同样的方法可以得到其他以 \<a>,\<b>,\<c>,\<e>,\<f> 为前缀的频繁序列。

![](https://upload-images.jianshu.io/upload_images/16911112-60f5246eeb1b6b35.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 5. 算法流程
* 输入：序列数据集 S 和支持度阈值 α
* 输出：所有满足支持度要求的频繁序列集

### 算法流程
1. 找出所有长度为1的前缀和对应的投影数据库；

2. 对长度为1的前缀进行计数，将支持度低于阈值α的前缀对应的项从数据集S删除，同时得到所有的频繁1项序列，i=1；

3. 对于每个长度为i满足支持度要求的前缀进行递归挖掘。

  * a. 找出前缀所对应的投影数据库。如果投影数据库为空，则递归返回。
  
  * b. 统计对应投影数据库中各项的支持度计数。如果所有项的支持度计数都低于阈值α，则递归返回。
  
  * c. 将满足支持度计数的各个单项和当前的前缀进行合并，得到若干新的前缀。
  
  * d. 令i=i+1，前缀为合并单项后的各个前缀，分别递归执行第3步。

## 6. 小结

&#8195;  PrefixSpan算法由于不用产生候选序列，且投影数据库缩小的很快，内存消耗比较稳定，作频繁序列模式挖掘的时候效果很高。

&#8195;  PrefixSpan运行时最大的消耗在递归的构造投影数据库。如果序列数据集较大，项数种类较多时，算法运行速度会有明显下降。因此有一些PrefixSpan的改进版算法都是在优化构造投影数据库这一块。比如使用伪投影计数。
