# 决策树
# 一、决策树简介
&#8195; **决策树**(**Decision Tree**)是一种逼近离散函数值的方法。它是一种典型的分类方法，首先对数据进行处理，利用归纳算法生成可读的规则和决策树，然后使用决策对新数据进行分析。本质上决策树是通过一系列规则对数据进行分类的过程。决策树学习常用的算法有ID3、C4.5与CART。

# 二、ID.3 决策树
## 1. 算法信息论基础
* 首先，我们需要熟悉信息论中熵的概念。熵度量了事物的不确定性，越不确定的事物，它的熵就越大。具体的，随机变量 $X$ 的熵的表达式如下：
$$H(X) = -\sum\limits_{i=1}^{n}p_i logp_i$$
其中 $n$ 代表 $X$ 的 $n$ 种不同的离散取值。而 $p_i$ 代表了 $X$ 取值为 $i$ 的概率，$log$ 为以 2 或者 $e$ 为底的对数。

&#8195; 举个例子，比如 $X$ 有 2 个可能的取值，而这两个取值各为 1/2 时 $X$ 的熵最大，此时 $X$ 具有最大的不确定性。此时值为：
$$H(X) = -(\frac{1}{2}log\frac{1}{2} + \frac{1}{2}log\frac{1}{2}) = log2$$

&#8195; 如果一个值概率大于 1/2，另一个值概率小于 1/2，则不确定性减少，对应的熵也会减少。比如一个概率 1/3，一个概率 2/3，则对应熵为：
$$H(X) = -(\frac{1}{3}log\frac{1}{3} + \frac{2}{3}log\frac{2}{3}) = log3 - \frac{2}{3}log2 < log2)$$

* 熟悉了一个变量 $X$ 的熵，很容易推广到多个变量的联合熵，这里给出两个变量 $X$ 和 $Y$ 的联合熵表达式：
$$H(X,Y) = -\sum\limits_{x_i \in X}\sum\limits_{y_i \in Y}p(x_i,y_i)logp(x_i,y_i)$$

* 有了联合熵，又可以得到条件熵的表达式 $H(X|Y)$，条件熵类似于条件概率，它度量了我们的 $X$ 在知道 $Y$ 以后剩下的不确定性。表达式如下：
$$H(X|Y) = -\sum\limits_{x_i \in X}\sum\limits_{y_i \in Y}p(x_i,y_i)logp(x_i|y_i) = \sum\limits_{j=1}^{n}p(y_j)H(X|y_j)$$ 
&#8195; 从上面的描述可以看出，它度量了 $X$ 在知道 $Y$ 以后不确定性减少程度，这个度量我们在信息论中称为互信息，记为 $I(X,Y)$，在决策树ID3算法中叫做信息增益。ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。

* 这样我们可以得出：
$$H(X,Y)=H(Y)+H(X|Y)=H(X)+H(Y|X)$$

&#8195; 如果难以理解，用下面这个图很容易明白他们的关系。左边的椭圆代表 $H(X)$，右边的椭圆代表 $H(Y)$，中间重合的部分就是我们的互信息或者信息增益$I(X,Y)$，左边的椭圆去掉重合部分就是 $H(X|Y)$，右边的椭圆去掉重合部分就是 $H(Y|X)$。两个椭圆的并就是 $H(X,Y)$。

![](https://upload-images.jianshu.io/upload_images/16911112-f7ad95963f9c197b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

* 基于上述的讨论，我们不禁会问，如果有 n 个随机变量处于一个随机系统中，那么我们获取其联合熵也是无关观察先后吗？答案是肯定的。为了说明原因，我们给出熵的链式法则：

    * 设随机变量 $X_1,X_2,...,X_n$ 服从 $p(x_1,x_2,...,x_n)$ ,则有：
$$H(X_1,X_2,...,X_n)=\sum_{i=1}^{n}H(X_i|X_{i-1},...,X_1)$$
    
    * 我们可以利用数学推导证明：
$$\begin{aligned}
H(X_1,X_2,...,X_n) & = -\sum_{x_1,...,x_n \in X_n}p(x_1,x_2,...,x_n)log\;p(x_1,...,x_n) \\[2ex]
 & = -\sum_{x_1,...,x_n \in X_n}p(x_1,x_2,...,x_n)log\;p(x_1,x_2,...,x_{n-1})p(x_n|x_1,...,x_{n-1}) \\[2ex] 
 & = -\sum_{x_1,...,x_n \in X_n}p(x_1,x_2,...,x_n)log\;p(x_1,...,x_{n-2})p(x_{n-1}|x_1,...,x_{n-2})p(x_n|x_1,...,x_{n-1}) \\[2ex]
 & = -\sum_{x_1,...,x_n \in X_n}p(x_1,x_2,...,x_n)log\;\prod_{i=1}^{n} p(x_i|x_{i-1},...,x_1) \\[2ex] 
 & = -\sum_{x_1,...,x_n \in X_n}p(x_1,x_2,...,x_n)\sum_{i=1}^{n}log\;p(x_i|x_{i-1},...,x_1) \\[2ex]
 & = \sum_{i=1}^{n}H(X_i|X_{i-1},...,X_1)
\end{aligned}$$
    
    * 从链式法则，我们可以更进一步得到，如果随机变量 $X_1,X_2,...,X_n$ 是独立的，那么联合熵则可以表示为：
$$H(X_1,X_2,...,X_n)=\sum_{i=1}^{n}H(X_i)$$
    
## 2. 算法思路
&#8195; ID3 算法就是用**信息增益**大小来判断当前节点应该用什么特征来构建决策树，**用计算出的信息增益最大的特征来建立决策树的当前节点**。这里我们举一个信息增益计算的具体的例子。比如我们有15个样本 $D$，输出为0或者1。其中有9个输出为1， 6个输出为0。 样本中有个特征 $A$，取值为 $A_1，A_2$ 和 $A_3$。在取值为 $A_1$ 的样本的输出中，有3个输出为1， 2个输出为0，取值为 $A_2$ 的样本输出中，2个输出为1，3个输出为0， 在取值为 $A_3$ 的样本中，4个输出为1，1个输出为0。

* 样本 $D$ 的熵为：
$$H(D) = -(\frac{9}{15}log_2\frac{9}{15} + \frac{6}{15}log_2\frac{6}{15}) = 0.971$$

* 样本 $D$ 在特征下的条件熵为：
$$\begin{aligned}
H(D|A) & = \frac{5}{15}H(D1) + \frac{5}{15}H(D2) + \frac{5}{15}H(D3) \\[3ex]
 & = -\frac{5}{15}(\frac{3}{5}log_2\frac{3}{5} + \frac{2}{5}log_2\frac{2}{5}) - \frac{5}{15}(\frac{2}{5}log_2\frac{2}{5} + \frac{3}{5}log_2\frac{3}{5}) -\frac{5}{15}(\frac{4}{5}log_2\frac{4}{5} + \frac{1}{5}log_2\frac{1}{5}) \\[3ex]
& = 0.888\end{aligned}$$

* 对应的信息增益为：
$$I(D,A) = H(D) - H(D|A) = 0.083$$

* 假设有多个特征 B、C，那么对应的信息增益为：
$$I(D,B) = H(D) - H(D|B) \\
I(D,C) = H(D) - H(D|C)$$

然后比较各个特征之间的信息增益，取信息增益最大值的特征为根节点。

### 算法过程
* 输入的是 $m$ 个样本，样本输出集合为 $D$，每个样本有 $n$ 个离散特征，特征集合即为 $A$；

* 输出为决策树 $T$。
* 算法过程：

    * a. 初始化信息增益的阈值 $ϵ$；
    
    * b. 判断样本是否为同一类输出 $D_i$ ，如果是则返回单节点树 $T$，标记类别为 $D_i$；
    
    * c. 判断特征是否为空，如果是则返回单节点树 $T$，标记类别为样本中输出类别 $D$ 实例数最多的类别；
    
    * d. 计算 $A$ 中的各个特征（一共n个）对输出 $D$ 的信息增益，选择信息增益最大的特征 $A_g$；
    
    * e. 如果 $A_g$ 的信息增益小于阈值 $ϵ$，则返回单节点树 $T$，标记类别为样本中输出类别 $D$ 实例数最多的类别；
    
    * f. 否则，按特征 $A_g$ 的不同取值 $A_{gi}$ 将对应的样本输出 $D$ 分成不同的类别 $D_i$，每个类别产生一个子节点，对应特征值为 $A_{gi}$。返回增加了节点的数 $T$；
    
    * g. 对于所有的子节点，令 $D=D_i,A=A−\lbrace A_g \rbrace$ 递归调用 b-f 步，得到子树 $T_i$ 并返回。

## 3. 算法的不足

ID3 算法虽然提出了新思路，但是还是有很多值得改进的地方。　　

* ID3 没有考虑连续特征，比如长度，密度都是连续值，无法在 ID3 运用。这大大限制了 ID3 的用途；

* ID3 采用信息增益大的特征优先建立决策树的节点。在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大；
* ID3 算法对于缺失值的情况没有做考虑；
* 没有考虑过拟合的问题。

---

# 三、C4.5决策树
## 1. 算法的改进
&#8195; 前面讲到 ID3 算法有四个主要的不足，一是不能处理连续特征，第二个就是用信息增益作为标准容易偏向于取值较多的特征，最后两个是缺失值处理的问和过拟合问题。

1. **对于第一个问题，不能处理连续特征， C4.5 的思路是将连续的特征离散化**。<br>
比如 $m$ 个样本的连续特征 $A$ 有 $m$ 个，从小到大排列为 $a_1,a_2,...,a_m$，则 C4.5 取相邻两样本值的平均数，一共取得 m-1 个划分点，其中第 i 个划分点 $T_i$ 表示为：$T_i = \frac{a_i+a_{i+1}}{2}$。对于这 m-1 个点，**分别计算以该点作为二元分类点时的信息增益，选择信息增益最大的点作为该连续特征的二元离散分类点**。比如取到的增益最大的点为 $a_t$，则小于 $a_t$ 的值为类别1，大于 $a_t$ 的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。

2. **对于第二个问题，信息增益作为标准容易偏向于取值较多的特征的问题**。<br>
我们引入一个信息增益比的变量 $I_R(X,Y)$，它是信息增益和特征熵的比值。表达式如下：
$$I_R(D,A) = \frac{I(A,D)}{H_A(D)}$$
其中 D 为样本特征输出的集合，A 为样本特征，对于特征熵 $H_A(D)$, 表达式如下：
$$H_A(D) = -\sum\limits_{i=1}^{n}\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$$
其中 n 为特征 $A$ 的类别数， $D_i$ 为特征 $A$ 的第 i 个取值对应的样本个数。$|D|$ 为样本个数。特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。

3. **对于第三个缺失值处理的问题**，主要需要解决的是两个问题：**一是在样本某些特征缺失的情况下选择划分的属性；二是选定了划分属性，对于在该属性上缺失特征的样本的处理**。

    * 对于第一个子问题，对于某一个有缺失特征值的特征 $A$。C4.5 的思路是：**将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值 $A$ 的数据 D1，另一部分是没有特征 $A$ 的数据 D2。 然后对于没有缺失特征 A 的数据集 D1 来和对应的 A 特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征 $A$ 缺失的样本加权后所占加权总样本的比例**。
    
    * 对于第二个子问题，可以**将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征 $A$ 的样本 a 之前权重为1，特征 $A$ 有3个特征值 $A1,A2,A3$。3个特征值对应的无缺失 A 特征的样本个数为2,3,4，则 a 同时划分入 $A1,A2,A3$，对应权重调节为 2/9, 3/9, 4/9**。

4. 对于过拟合问题，C4.5 引入了正则化系数进行初步的剪枝。

## 2. 算法的不足与思考

C4.5 虽然改进或者改善了 ID3算法的几个主要的问题，仍然有优化的空间。

* 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，**一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝**。

* C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。

* C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。

* C4.5由于使用了熵模型，里面有大量的耗时的对数运算，如果是连续值还有大量的排序运算。

---

# 四、CART决策树
## 1. CART分类树算法
### 1.1 最优特征选择方法
&#8195; 我们知道，在 **ID3算法**中我们使用了**信息增益**来选择特征，信息增益大的优先选择。在 **C4.5算法**中，采用了**信息增益比**来选择特征，以减少信息增益容易选择特征值多的特征的问题。但是无论是ID3还是C4.5，都是基于信息论的熵模型的，这里面会涉及大量的对数运算。CART分类树算法使用**基尼系数来**代替信息增益比，**基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好**。

具体的，在分类问题中，假设有 K 个类别，第 k 个类别的概率为 $p_k$， 则基尼系数的表达式为：
$$Gini(p) = \sum\limits_{k=1}^{K}p_k(1-p_k) = 1- \sum\limits_{k=1}^{K}p_k^2$$

如果是二类分类问题，计算就更加简单了，如果属于第一个样本输出的概率是 p，则基尼系数的表达式为：
$$Gini(p) = 2p(1-p)$$

对于个给定的样本 D，假设有 K 个类别，第 k 个类别的数量为 $C_k$，则样本 D 的基尼系数表达式为：
$$Gini(D) = 1-\sum\limits_{k=1}^{K}(\frac{|C_k|}{|D|})^2$$

特别的，对于样本 D，如果根据特征 A 的某个值 a，把 D 分成 $D_1$ 和 $D_2$ 两部分，则在特征 A 的条件下，D 的基尼系数表达式为：
$$Gini(D,A) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)$$

&#8195; 比较下基尼系数表达式和熵模型的表达式，发现二次运算比对数简单很多，尤其是二类分类的计算，更加简单。对于二类分类，基尼系数和熵之半的曲线如下：

![](https://upload-images.jianshu.io/upload_images/16911112-ac1bea342bfbae0e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

&#8195; 从上图可以看出，基尼系数和熵之半的曲线非常接近，仅仅在45度角附近误差稍大。因此，基尼系数可以做为熵模型的一个近似替代，而 CART 分类树算法就是使用基尼系数来选择决策树的特征。同时，为了进一步简化，CART 分类树算法**每次仅仅对某个特征的值进行二分**，而不是多分，这样 CART 分类树算法建立起来的是二叉树，而不是多叉树。这样一可以进一步简化基尼系数的计算，二可以建立一个更加优雅的二叉树模型。

### 1.2 对于连续特征和离散特征处理的改进

* 对于 CART 分类树连续值的处理问题，其思想和 C4.5 是相同的，都是**将连续的特征离散化**。唯一的区别在于在选择划分点时的度量方式不同，C4.5 使用的是信息增益比，则 CART 分类树使用的是基尼系数。具体的思路如下：

    * 比如 m 个样本的连续特征 A 有 m 个，从小到大排列为 $a_1,a_2,...,a_m$，则 CART 算法取相邻两样本值的平均数，一共取得 m-1 个划分点，其中第 i 个划分点 $T_i$ 表示为：$T_i = \frac{a_i+a_{i+1}}{2}$。**对于这 m-1 个点，分别计算以该点作为二元分类点时的基尼系数，选择基尼系数最小的点作为该连续特征的二元离散分类点。比如取到的基尼系数最小的点为 $a_t$，则小于 $a_t$ 的值为类别1，大于 $a_t$ 的值为类别2，这样我们就做到了连续特征的离散化**。要注意的是，与 ID3 或者 C4.5 处理离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。

* 对于CART分类树离散值的处理问题，采用的思路是**不停的二分离散特征**。

    * 如果某个特征 A 被选取建立决策树节点，如果它有 A1,A2,A3 三种类别，CART 分类树采用的是不停的二分。**CART 分类树会考虑把 A 分成 {A1} 和 {A2,A3}, {A2} 和 {A1,A3}, {A3} 和 {A1,A2} 三种情况，找到基尼系数最小的组合，比如 {A2} 和 {A1,A3}，然后建立二叉树节点，一个节点是 A2 对应的样本，另一个节点是 {A1,A3} 对应的节点**。同时，由于这次没有把特征 A 的取值完全分开，后面我们还有机会在子节点继续选择到特征 A 来划分 A1 和 A3。这和 ID3 或者 C4.5 不同，在 ID3 或者 C4.5 的一棵子树中，离散特征只会参与一次节点的建立。

### 1.3 建立算法的具体流程
* 算法输入是训练集 D，基尼系数的阈值，样本个数阈值。<br>

* 输出是决策树 T。<br>

* 我们的算法从根节点开始，用训练集递归的建立 CART 树。

    * a. 对于当前节点的数据集为 D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归。
    
    * b. 计算样本集 D 的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归。
    
    * c. 计算当前节点现有的各个特征的各个特征值对数据集 D 的基尼系数，对于离散值和连续值的处理方法和基尼系数的计算见 <font color=Blue>[*1.2*](###_12-对于连续特征和离散特征处理的改进)。</font>
    
    * d. 在计算出来的各个特征的各个特征值对数据集 D 的基尼系数中，选择基尼系数最小的特征 A 和对应的特征值 a。根据这个最优特征和最优特征值，把数据集划分成两部分 D1 和 D2，同时建立当前节点的左右节点，左节点的数据集 D 为 D1，右节点的数据集 D 为 D2.
    
    * e. 对左右的子节点递归的调用 a~e 步，生成决策树。

* 对于生成的决策树做预测的时候，假如测试集里的样本A落到了某个叶子节点，而节点里有多个训练样本，则对于A的类别预测采用的是这个叶子节点里概率最大的类别。

## 2. CART回归树算法
&#8195; 首先，我们要明白，什么是回归树，什么是分类树。两者的区别在于样本输出，如果样本输出是离散值，那么这是一颗分类树。如果果样本输出是连续值，那么那么这是一颗回归树。除了概念的不同，CART回归树和CART分类树的建立和预测的区别主要有下面两点：
* 连续值的处理方法不同
* 决策树建立后做预测的方式不同。

&#8195; 对于连续值的处理，我们知道 **CART分类树**采用的是**用基尼系数的大小来度量特征的各个划分点的优劣情况**。这比较适合分类模型，但是**对于回归模型，我们使用了均方差的度量方式**，CART 回归树的度量目标是：**对于任意划分特征 A，对应的任意划分点 s 两边划分成的数据集 D1 和 D2，求出使 D1 和 D2 各自集合的均方差最小，同时 D1 和 D2 的均方差之和最小所对应的特征和特征值划分点**。表达式为：
$$\underbrace{min}_{A,s}\Bigg[\underbrace{min}_{c_1}\sum\limits_{x_i \in D_1(A,s)}(y_i - c_1)^2 + \underbrace{min}_{c_2}\sum\limits_{x_i \in D_2(A,s)}(y_i - c_2)^2\Bigg]$$
其中，$c_1$ 为 $D_1$ 数据集的样本输出均值，$c_2$ 为 $D_2$ 数据集的样本输出均值。

&#8195; 对于决策树建立后做预测的方式，回归树采用的是**用最终叶子的均值或者中位数来预测输出结果**。

## 3. CART树算法的剪枝
&#8195; CART 回归树和 CART 分类树的剪枝策略除了在度量损失的时候一个使用**均方差**，一个使用**基尼系数**，算法基本完全一样。

&#8195; 由于决策时算法很容易对训练集过拟合，而导致泛化能力差，为了解决这个问题，我们需要对 CART 树进行剪枝，即类似于线性回归的正则化，来增加决策树的泛化能力。CART 采用的办法是后剪枝法，即先生成决策树，然后产生所有可能的剪枝后的 CART 树，然后使用交叉验证来检验各种剪枝的效果，选择泛化能力最好的剪枝策略。也就是说，CART 树的剪枝算法可以概括为两步:

* 第一步是从原始决策树生成各种剪枝效果的决策树；

* 第二部是用交叉验证来检验剪枝后的预测能力，选择泛化预测能力最好的剪枝后的树作为最终的 CART 树。

![](https://upload-images.jianshu.io/upload_images/16911112-c8a6d2012753a3fd.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

1. 首先我们看看剪枝的损失函数度量，在剪枝的过程中，对于任意的一刻子树 $T$，其损失函数为：
$$C_{\alpha}(T_t) = C(T_t) + \alpha |T_t|$$
&#8195; 其中，α 为正则化参数，这和线性回归的正则化一样。$C(T_t)$ 为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。$|T_t|$ 是子树 $T$ 的叶子节点的数量。

    * 当 α=0 时，即没有正则化，原始的生成的 CART 树即为最优子树。
    
    * 当 α=∞ 时，即正则化强度达到最大，此时由原始的生成的 CART 树的根节点组成的单节点树为最优子树。
    
&#8195; 当然，这是两种极端情况。一般来说，α 越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的 α，一定存在使损失函数 $C_α(T)$ 最小的唯一子树。

2. 对于位于节点 t 的任意一颗子树 $T_t$，如果没有剪枝，它的损失是：
$$C_{\alpha}(T_t) = C(T_t) + \alpha |T_t|$$
如果将其剪掉，仅仅保留根节点，则损失是：
$$C_{\alpha}(T) = C(T) + \alpha$$
当 α=0 或者 α 很小时，$C_α(T_t)<C_α(T) $，当 α 增大到一定的程度时：
$$C_{\alpha}(T_t) = C_{\alpha}(T)$$
当 α 继续增大时不等式反向，也就是说，如果满足下式：
$$\alpha = \frac{C(T)-C(T_t)}{|T_t|-1}$$
$T_t$ 和 $T$ 有相同的损失函数，但是 $T$ 节点更少，因此可以对子树 $T_t$ 进行剪枝，也就是将它的子节点全部剪掉，变为一个叶子节点 $T$。

3. 最后我们看看 CART 树的交叉验证策略。上面我们讲到，可以计算出每个子树是否剪枝的阈值 α，如果我们把所有的节点是否剪枝的值 α 都计算出来，然后分别针对不同的α所对应的剪枝后的最优子树做交叉验证。这样就可以选择一个最好的 α，有了这个 α，我们就可以用对应的最优子树作为最终结果。

### CART树的剪枝算法
* 输入是 CART 树建立算法得到的原始决策树 $T_0$，输出是最优决策子树 $T_α$。算法主要过程如下：

    * 1. 初始化 $α_{min}=∞,k=0,T=T_0$， 最优子树集合 $ω={T}$；
    
    * 2. 从叶子节点开始自下而上计算各内部节点 t 的训练误差损失函数 $C_α(T_t)$（回归树为均方差，分类树为基尼系数）, 叶子节点数 $|T_t|$，以及正则化阈值 $\alpha= min\{\frac{C(T)-C(T_t)}{|T_t|-1}, \alpha_{min}\}$，$\alpha_{min}= \alpha$；
    
    * 3. $\alpha_k = \alpha_{min}$；
    
    * 4. 自上而下的访问子树 t 的内部节点，如果 $\frac{C(T)-C(T_t)}{|T_t|-1} \leq \alpha_k$ 时，进行剪枝。并决定叶节点 t 的值。如果是分类树，则是概率最高的类别，如果是回归树，则是所有样本输出的均值。这样得到 $α_k$ 对应的最优子树 $T_k$；
    
    * 5. 最优子树集合 $\omega=\omega \cup T_k$；
    
    * 6. 如果 ω 不为空，则 $k=k+1,T=T_k$，回到步骤2递归执行，否则就已经得到了所有的可选最优子树集合 ω；
    
    * 7. 采用交叉验证在 ω 选择最优子树 $T_α$。

## 4. CART算法小结

&#8195; CART算法相比 C4.5算法的分类方法，采用了简化的二叉树模型，同时特征选择采用了近似的基尼系数来简化计算。当然 CART树最大的好处是还可以做回归模型，这个 C4.5没有。下表给出了 ID3，C4.5 和 CART 的一个比较总结。

|算法|支持模型|树结构|特征选择|连续值处理|缺失值处理|剪枝|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
ID3分类|多叉树|信息增益|不支持|不支持|不支持|不支持
C4.5|分类|多叉树|信息增益比|支持|支持|支持
CART|分类，回归|二叉树|基尼系数，均方差|支持|支持|支持

CART算法主要的缺点如下：

* 无论是 ID3、C4.5 还是 CART，在做特征选择的时候都是选择最优的一个特征来做分类决策，但是大多数，分类决策不应该是由某一个特征决定的，而是应该由一组特征决定的，这样决策得到的决策树更加准确，这个决策树叫做**多变量决策树(multi-variate decision tree)**。在选择最优特征的时候，多变量决策树不是选择某一个最优特征，而是选择最优的一个特征线性组合来做决策。

* 如果样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习里面的随机森林之类的方法解决。

---

# 五、总结
## 三种算法的差异
1. **划分标准的差异**：ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。

2. **使用场景的差异**：ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；

3. **样本数据的差异**：ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；

4. **样本特征的差异**：ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征；

5. **剪枝策略的差异**：ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。

## 优点
* 简单直观，生成的决策树很直观。

* 基本不需要预处理，不需要提前归一化，处理缺失值；
* 使用决策树预测的代价是 O(log2m)， m 为样本数；
* 既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值；
* 可以处理多维度输出的分类问题；
* 相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释；
* 可以交叉验证的剪枝来选择模型，从而提高泛化能力；
* 对于异常点的容错能力好，健壮性高。

## 缺点
* 决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进；

* 决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决；
* 寻找最优的决策树是一个 NP 难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善；
* 有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决；
* 如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。

---

# 六、决策树题目
## 1. 简述决策树的原理
* 决策树是一种树结构，从根节点出发，每个分支都将训练数据划分成了互不相交的子集。分支的划分可以以单个特征为依据，也可以以特征的线性组合为依据。决策树可以解决回归和分类问题，在预测过程中，一个测试数据会依据已经训练好的决策树到达某一叶子节点，该叶子节点即为回归或分类问题的预测结果。

* 从概率论的角度理解，决策树是定义在特征空间和类空间上的条件概率分布。每个父节点可以看作子树的先验分布，子树则为父节点在当前特征划分下的后验分布。

## 2. 谈谈对信息增益和信息增益率的理解
* **信息熵**：信息熵用来度量样本集合的纯度，在ID3算法中，选择的是信息增益来进行特征选择，信息增益大的特征优先选择。公式如下：
$$H(D) = -\sum_{i=k}^{k}\frac{|C_k|}{|D|}\;log_2 \frac{|C_k|}{|D|}$$
其中 $C_k$ 表示集合 D 中属于第 k 类样本的样本子集。信息熵值越小，D 的纯度越高。

* **条件熵**：针对某个特征 A，对于数据集 D 的条件熵 H(D|A) 为：
$$H(D|A) = -\sum_{i=1}^{n}\frac{|D_i|}{|D|}(\sum_{i=1}^{n}\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|})$$

* **信息增益**：信息增益用来描述一次划分之后纯度的提升有多大，用不同的属性划分样本，会得到不同的信息增益。在 ID3 决策树算法中，我们取能使信息增益最大，即划分后纯度提升最大的属性作为当前决策树的划分属性。**信息增益 = 信息熵 - 条件熵**，信息增益的公式如下：
$$Gain(D,A) = H(D) - H(D|A)$$

* **信息增益率**：使用信息增益当作 cost function 会对 可取值数目较多 的属性有所偏好，使用信息增益率可以减小这种偏好，缺点是信息增益率会偏向取值较少的特征。
$$Gain_{ratio}(D,A) = \frac{Gain(D,A)}{H_A(D)}\\
H_A(D) = \sum_{i=1}^{n}\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$$
$H_A(D)$ 称为特征 A 的固有值。信息增益率对可取值较少的特征有所偏好（分母越小，整体越大），因此 C4.5 并不是直接用增益率最大的特征进行划分，而是使用一个**启发式方法**：先从候选划分特征中找到信息增益高于平均值的特征，再从中选择增益率最高的。

## 3. 什么是基尼系数？
* 基尼指数（基尼不纯度）= 样本被选中的概率 * 样本被分错的概率，基尼系数的公式为：
$$Gini(D) = \sum_{i=1}^{k} p_k(1-p_k) = \sum_{i=1}^{k} 1 - p_k^2$$

* 基尼系数的意义是从数据集D中随机抽取两个样本类别标识不一致的概率。基尼系数越小，数据集的纯度越高。相比于信息增益，信息增益比等作为特征选择方法，基尼系数省略了对数计算，运算量比较小，也比较容易理解，所以CART树选择使用基尼系数用来做特征选择

## 4. 简述决策树构建过程
1. 构建根结点：将所有训练数据放在根结点。

2. 选择一个最优特征，根据这个特征将训练数据分割成子集，使得各个子集有一个在当前条件下最好的分类。

    * 若这些子集已能够被基本正确分类，则将该子集构成叶结点。
    
    * 若某个子集不能够被基本正确分类，则对该子集选择新的最优的特征，继续对该子集进行分割，构建相应的结点。

3. 如此递归下去，直至所有训练数据子集都被基本正确分类，或者没有合适的特征为止。

## 5. 如何对决策树剪枝？
剪枝是防止决策树过拟合的方法。一棵完全生长的决策树很可能失去泛化能力，因此需要剪枝。

1. 剪枝的策略：
    * 剪枝分为 预剪枝 和 后剪枝 两种，预剪枝是在构建决策树时抑制它的生长，后剪枝是决策树生长完全后再对叶子节点进行修剪。

2. 预剪枝-预剪枝的方法有：

    * 设置一个树的最大高度/深度或者为树设置一个最大节点数，达到这个值即停止生长；
      
    * 对每个叶子节点的样本数设置最小值，生长时叶子节点样本数不能小于这个值；    
    * 判断每次生长对系统性能是否有增益。对于一个决策树，每次生长前，可以判断生长后系统在验证集上准确度是否提升，如果经过一次生长，系统在验证集上的准确度降低了，那么中止这次生长。

3. 后剪枝-后剪枝方法是对一棵已经完全生长的决策树进行剪枝，后剪枝的主要方法有：

    * 错误率降低剪枝（Reduced-Error Pruning）
   
    * 悲观剪枝（Pessimistic Error Pruning）   
    * 代价复杂度剪枝（Cost-Complexity Pruning）
    
我们重点介绍第一种。错误率降低剪枝的方法比较直观，从下至上遍历所有非叶子节点的子树，每次把子树剪枝（所有数据归到该节点，将数据中最多的类设为结果），与之前的树在验证集上的准确率进行比较，如果有提高，则剪枝，否则不剪，直到所有非叶子节点被遍历完。

4. 预剪枝和后剪枝的优缺点比较：

    * 时间成本方面，预剪枝在训练过程中即进行剪枝，后剪枝要在决策树完全生长后自底向上逐一考察。显然，后剪枝训练时间更长。预剪枝更适合解决大规模问题。

    * 剪枝的效果上，预剪枝的常用方法本质上是基于贪心的思想，但贪心法却可能导致欠拟合，后剪枝的欠拟合风险很小，泛化性能更高。

## 6. 决策树的目标函数是什么？
1. 生成过程：（从局部出发）递归地选择最佳特征构建决策树，相当于用 极大似然法 进行概率模型的选择；

2. 剪枝过程：（从整体出发）相当于极小化决策树的目标函数（损失函数）。设树 T 的叶结点个数为 |T|，t 是树 T 的叶结点，该叶结点有 $N_t$ 个样本点，其中 k 类的样本点有 $N_{tk}$ 个，k=1,2,...,K，$H_t(T)$ 为叶结点 t 上的经验嫡，$\alpha\geq0$ 为参数，则决策树学习的损失函数可以定义为：
$$C_{\alpha}(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T| \\
H_t(T) = -\sum_{k}\frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t} \\
C(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_{tk}log\frac{N_{tk}}{N_t}$$
C(T) 表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T| 表示模型复杂度，参数 $\alpha\geq0$ (正则参数)控制两者之间的影响。剪枝，就是当 $\alpha$ 确定时，选择损失函数最小的模型，即损失函数最小的子树。损失函数正好表示了对模型的复杂度和训练数据的拟合两者的平衡。

## 7. 决策树如何防止过拟合？
1. 剪枝
    * 预剪枝：在分裂节点的时候设计比较苛刻的条件，如不满足则直接停止分裂（这样干决策树无法到最优，也无法得到比较好的效果）；

    * 后剪枝：在树建立完之后，用单个节点代替子树，节点的分类采用子树中主要的分类（这种方法比较浪费前面的建立过程）。
    
2. 使用交叉验证。

3. 建立随机森林。

## 8. 如果有异常值或者数据分布不均匀，会对决策树有什么影响？

* 如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。

## 9. 手动构建 CART的回归树的前两个节点，给出公式每一步的公式推导
1. 对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归。

2. 计算样本集D的平方误差，如果平方误差小于阈值，则返回决策树子树，当前节点停止递归。

## 10. 决策树和其他模型相比有什么优点？
* 相对于其他数据挖掘算法，决策树在以下几个方面拥有优势：

    * 决策树易于理解和实现. 人们在通过解释后都有能力去理解决策树所表达的意义；

    * 对于决策树，数据的准备往往是简单或者是不必要的 . 其他的技术往往要求先把数据一般化，比如去掉多余的或者空白的属性；
    * 能够同时处理数据型和常规型属性。其他的技术往往要求数据属性的单一；
    * 在相对短的时间内能够对大型数据源做出可行且效果良好的结果；
    * 对缺失值不敏感；
    * 可以处理不相关特征数据；
    * 效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。
    
* 决策树的缺点

    * 对连续性的字段比较难预测；

    * 对有时间顺序的数据，需要很多预处理的工作；
    * 当类别太多时，错误可能就会增加的比较快；
    * 一般的算法分类的时候，只是根据一个字段来分类；
    * 在处理特征关联性比较强的数据时表现得不是太好。


## 11. 如果特征很多，决策树中最后没有用到的特征一定是无用吗？
* 不是无用的，从两个角度考虑，一是特征替代性，如果可以已经使用的特征A和特征B可以提点特征C，特征C可能就没有被使用，但是如果把特征C单独拿出来进行训练，依然有效。

* 其二，决策树的每一条路径就是计算条件概率的条件，前面的条件如果包含了后面的条件，只是这个条件在这棵树中是无用的，如果把这个条件拿出来也是可以帮助分析数据。

## 12. 决策树和随机森林的区别是什么？
* 相同点：

    * 都是由多棵树组成的，都是集成学习算法；
    
    * 最终的结果都是由多颗树一起决定。

* 不同点：

    * 组成随机森林的树可以是分类树，也可以是回归树，但是GBDT只能由回归树组成；
    
    * 组成随机森林的树可以并行生成，但是组成GBDT的树只能串行生成；    
    * 对于最终的输出结果，随机森林采用多数投票；而GBDT是将所有的结果累加起来，或者加权起来；    
    * 随机森林对异常值不敏感，而GBDT对异常值非常敏感；    
    * 随机森林通过减小方差来提高性能，GBDT通过减小偏差来提高性能。