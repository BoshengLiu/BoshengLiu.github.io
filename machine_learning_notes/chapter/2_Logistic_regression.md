# 逻辑回归（未完善）
&#8195;  **Logistic Regression** 虽然被称为回归，但其实际上是分类模型，并常用于二分类。Logistic 回归的本质是：**假设数据服从这个分布，然后使用极大似然估计做参数的估计**。

# 一、逻辑回归的模型
* 对线性回归的结果做一个在函数 $g$ 上的转换，可以变化为逻辑回归。这个函数 $g$ 在逻辑回归中我们一般取为**sigmoid函数**，形式如下：
$$g(z) = \frac{1}{1+e^{-z}}$$

* 它有一个非常好的性质，即当 $z$ 趋于正无穷时，$g(z)$ 趋于1，而当 $z$ 趋于负无穷时，$g(z)$ 趋于0，这非常适合于我们的分类概率模型。另外，它还有一个很好的导数性质：
$$g'(z) = g(z)(1-g(z))$$

* 如果我们令 $g(z)$ 中的 $z$ 为：${z = x\theta}$，这样就得到了二元逻辑回归模型的一般形式：
$$h_{\theta}(x) = \frac{1}{1+e^{-x\theta}}$$

* 其中 $x$ 为样本输入，$h_θ(x)$ 为模型输出，可以理解为某一分类的概率大小。而 $θ$ 为分类模型的要求出的模型参数。对于模型输出 $h_θ(x)$，我们让它和我们的二元样本输出 $y$（假设为 0 和 1）有这样的对应关系：
  * 如果 $h_θ(x)>0.5$，即 $x_θ>0$, 则 $y$ 为 1。
  * 如果 $h_θ(x)<0.5$，即 $x_θ<0$, 则 $y$ 为 0。
  * $y=0.5$ 是临界情况，此时 $x_θ=0$, 从逻辑回归模型本身无法确定分类。

* $h_θ(x)$ 的值越小，而分类为 0 的的概率越高，反之，值越大的话分类为 1 的的概率越高。如果靠近临界点，则分类准确率会下降。其矩阵形式为：
$$h_{\theta}(x) = \frac{1}{1+e^{-x\theta}}$$
其中 $h_θ(x)$ 为模型输出，为 $m \times 1$ 的向量。$x$ 为样本特征矩阵，为 $m \times n$ 的向量。$θ$ 为分类的模型系数，为 $n \times 1$ 的向量。

---

# 二、损失函数
&#8195;  逻辑回归模型的数学形式确定后，剩下就是如何去求解模型中的参数。在统计学中，常常使用极大似然估计法来求解，即找到一组参数，使得在这组参数下，我们的数据的似然度（概率）最大。

* 设：
$$P(y=1|x,\theta ) = h_{\theta}(x)$$

$$P(y=0|x,\theta ) = 1- h_{\theta}(x)$$

* 根据联合概率公式，将上式合成为：
$$P(y|x,\theta ) = h_{\theta}(x)^y(1-h_{\theta}(x))^{1-y}$$
其中，$y$ 的取值只能是 0 或者 1。

* 得到了 $y$ 的概率分布函数表达式，我们就可以用似然函数最大化来求解我们需要的模型系数 θ。为了方便求解，这里我们用对数似然函数最大化，对数似然函数取反即为我们的损失函数 $J(θ)$。似然函数的代数表达式为：
$$L(\theta) = \prod\limits_{i=1}^{m}(h_{\theta}(x_i))^{y_i}(1-h_{\theta}(x_i))^{1-y_i}$$
其中 $m$ 为样本的个数。

* 对似然函数对数化取反的表达式，即损失函数表达式为：
$$J(\theta) = -lnL(\theta) = -\sum\limits_{i=1}^{m}(y_i log(h_{\theta}(x_i))+ (1-y_i)log(1-h_{\theta}(x_i)))$$

* 损失函数的矩阵形式：
$$J(\theta) = -Y^Tlogh_{\theta}(X) - (E-Y)^T log(E-h_{\theta}(X))$$
其中 $E$ 为单位矩阵。

---

# 三、损失函数的优化
&#8195;  对于二元逻辑回归的损失函数极小化，有比较多的方法，最常见的有梯度下降法，坐标轴下降法，牛顿法等。这里将推导出梯度下降法中 $θ$ 每次迭代的公式。

* 对于：
$$J(\theta) = -Y^T logh_{\theta}(X) - (E-Y)^T log(E-h_{\theta}(X))$$

* 我们用 $J(θ)$ 对 $θ$ 向量求导可得：
$$\frac{\partial}{\partial\theta}J(\theta) 
= X^T[\frac{1}{h_{\theta}(X)}\odot h_{\theta}(X)\odot (E-h_{\theta}(X))\odot (-Y)]+X^T[\frac{1}{E-h_{\theta}(X)}\odot h_{\theta}(X)\odot (E-h_{\theta}(X))\odot (E-Y)]$$

* 这一步我们用到了向量求导的链式法则，和下面三个基础求导公式的矩阵形式：
$$\frac{\partial logx}{\partial x} = \frac{1}{x}$$

$$\frac{\partial g(z)}{\partial z} = g(z)(1-g(z))$$

$$\frac{\partial x\theta}{\partial \theta} =x$$
&#8195; 其中，$g(z)$ 为 `sigmoid` 函数。

* 我们对求导公式进行化简可得：
$$\frac{\partial}{\partial\theta}J(\theta) = X^T(h_{\theta}(X) - Y )$$

* 从而在梯度下降法中每一步向量 $θ$ 的迭代公式如下：
$$\theta = \theta - \alpha X^T(h_{\theta}(X) - Y )$$
其中，$α$ 为梯度下降法的步长。

---

# 四、逻辑回归的正则化
&#8195;  逻辑回归也会面临过拟合问题，所以我们也要考虑正则化。常见的有 L1 正则化和 L2 正则化。逻辑回归的 L1 正则化的损失函数表达式如下，相比普通的逻辑回归损失函数，增加了 L1 的范数做作为惩罚，超参数 α 作为惩罚系数，调节惩罚项的大小。

## 1. L1正则化损失函数表达式如下：
$$J(\theta) = -Y^T \bullet logh_{\theta}(X) - (E-Y)^T\bullet log(E-h_{\theta}(X)) +\alpha ||\theta||_1$$
其中 $||\theta||_1$ 为 $θ$ 的 L1 范数。逻辑回归的 L1 正则化损失函数的优化方法常用的有坐标轴下降法和最小角回归法。

## 2. L2正则化损失函数表达式如下：
$$J( \theta ) = -Y^T \bullet logh_{\theta}(X) - (E-Y)^T\bullet log(E-h_{\theta}(X)) + \frac{1}{2}\alpha||\theta||_2^2$$
其中 $|| \theta||_2$ 为 $θ$ 的 L2 范数。逻辑回归的 L2 正则化损失函数的优化方法和普通的逻辑回归类似。

## 3. L1正则化和L2正则化的区别
* 1.两者引入的关于模型参数的先验知识不一样。L1 正则化中 $w$ 服从零均值拉普拉斯分布；L2 正则化中 $w$ 服从零均值正态分布。

* 2.L1 偏向于使模型参数变得稀疏（但实际上并不那么容易），L2 偏向于使模型每一个参数都很小，但是更加稠密，从而防止过拟合。

---

# 五、逻辑回归问题
## 1. 为什么逻辑回归的输入要先离散化/Onehot
* 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。

* 离散化后的特征对异常数据有很强的[*鲁棒性*](https://baike.baidu.com/item/%E9%B2%81%E6%A3%92%E6%80%A7)：比如一个特征是年龄>30是 1，否则 0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。
* 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为 $N$ 个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。
* 离散化后可以进行特征交叉，由 $M+N$ 个变量变为 $M*N$ 个变量，进一步引入非线性，提升表达能力。
* 特征离散化后，模型会更稳定。比如如果对用户年龄离散化，20-30 作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。

## 2. 为什么逻辑回归不适合用MSE？
* 假设目标函数是`MSE`而不是交叉熵，即：
$$L=\frac{(y-\hat{y})^2}{2}，\frac{\partial L}{\partial w}=(\hat{y}-y)\sigma'(w\cdot x)x$$

* 这里 `sigmoid` 的导数项：
$$\sigma'(w\cdot x)=w\cdot x(1-w\cdot x)$$

* 根据 $w$ 的初始化，导数值可能很小（想象一下`sigmoid`函数在输入较大时的梯度）而导致收敛变慢，而训练途中也可能因为该值过小而提早终止训练（参考梯度消失）。

* 另一方面，logloss的梯度如下，当模型输出概率偏离于真实概率时，梯度较大，加快训练速度，当拟合值接近于真实概率时训练速度变缓慢，没有`MSE`的问题。
$$g=\sum_{i=1}^N x_i(y_i-p(x_i))$$

---

# 六、LR和SVM的比较
### 共同点：
* 1. 都是有监督分类方法，判别模型（直接估计 $y=f(x)$ 或 $p(y|x)$）

* 2. 都是线性分类方法（指不用核函数的情况）

### 不同点：
* 1. loss不同。LR是交叉熵，SVM 是Hinge loss，最大化函数间隔；

* 2. LR 决策考虑所有样本点，SVM 决策仅仅取决于支持向量；
* 3. LR 受数据分布影响，尤其是样本不均衡时影响大，要先做平衡，SVM不直接依赖于分布；
* 4. LR 可以产生概率，SVM 不能；
* 5. LR 不依赖样本之间的距离，SVM是基于距离的；
* 6. LR 是经验风险最小化，需要另外加正则，SVM自带结构风险最小化不需要加正则项；
* 7. LR 一般不用核函数，因为每两个点之间都要做内积，而 SVM 只需要计算样本和支持向量的内积，计算量更小；
* 8. 小数据集 SVM 比 LR 好，大数据 SVM 计算复杂，LR 简单且能够在线并行训练，因此更多用 LR。

---

# 七、LR和线性回归的区别
1. **线性回归要求输入x服从正态分布，LR没有要求**。</br>
线性回归的目标函数是MSE，用最小二乘法训练参数，原因正是它假设了误差服从正态分布，而误差与输入直接线性相关，因此即要求输入服从正态分布。

2. **线性回归要求输入x和输出y是连续型数值，LR要求y是分类型变量**。

3. **线性回归要求x和y呈线性关系，LR没有要求**。</br>
线性回归和LR都是线性模型，线性回归本质是回归不是判别，它要做的就是拟合样本构成的直线，LR是判别模型，要求x线性可分，但x和y并非是线性关系。

4. **线性回归直接分析x和y的关系，LR分析y取某个值的概率和x的关系**。

