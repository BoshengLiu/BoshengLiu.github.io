# 逻辑回归
# 一、模型介绍
&#8195;  我们知道，线性回归的模型是求出输出特征向量Y和输入样本矩阵X之间的线性关系系数θ，满足Y=Xθ。此时我们的Y是连续的，所以是回归模型。如果我们想要Y是离散的话，怎么办呢？一个可以想到的办法是，我们对于这个Y再做一次函数转换，变为g(Y)。如果我们令g(Y)的值在某个实数区间的时候是类别A，在另一个实数区间的时候是类别B，以此类推，就得到了一个分类模型。如果结果的类别只有两种，那么就是一个二元分类模型了。

&#8195;  Logistic Regression 虽然被称为回归，但其实际上是分类模型，并常用于二分类。Logistic 回归的本质是：**假设数据服从这个分布，然后使用极大似然估计做参数的估计**。

---

# 二、二元逻辑回归的模型
* 对线性回归的结果做一个在函数g上的转换，可以变化为逻辑回归。这个函数g在逻辑回归中我们一般取为**sigmoid函数**，形式如下：
$$g(z) = \frac{1}{1+e^{-z}}$$

* 它有一个非常好的性质，即当z趋于正无穷时，g(z)趋于1，而当z趋于负无穷时，g(z)趋于0，这非常适合于我们的分类概率模型。另外，它还有一个很好的导数性质：
$$g^{'}(z) = g(z)(1-g(z))$$

* 如果我们令g(z)中的z为：${z = x\theta}$，这样就得到了二元逻辑回归模型的一般形式：
$$h_{\theta}(x) = \frac{1}{1+e^{-x\theta}}$$

* 其中x为样本输入，$h_θ(x)$为模型输出，可以理解为某一分类的概率大小。而θ为分类模型的要求出的模型参数。对于模型输出$h_θ(x)$，我们让它和我们的二元样本输出y（假设为0和1）有这样的对应关系：
  * 如果$hθ_(x)>0.5 $，即$x_θ>0$, 则y为1。
  * 如果$h_θ(x)<0.5$，即$x_θ<0$, 则y为0。
  * y=0.5是临界情况，此时$x_θ=0$, 从逻辑回归模型本身无法确定分类。

* $h_θ(x)$的值越小，而分类为0的的概率越高，反之，值越大的话分类为1的的概率越高。如果靠近临界点，则分类准确率会下降。其矩阵形式为：
$$h_{\theta}(X) = \frac{1}{1+e^{-X\theta}}
$$其中$h_θ(X)$为模型输出，为 $m \times 1$的向量。X为样本特征矩阵，为$m \times n$的向量。θ为分类的模型系数，为$n \times 1$的向量。

---

# 三、损失函数
&#8195;  逻辑回归模型的数学形式确定后，剩下就是如何去求解模型中的参数。在统计学中，常常使用极大似然估计法来求解，即找到一组参数，使得在这组参数下，我们的数据的似然度（概率）最大。

* 设：
$$P(y=1|x,\theta ) = h_{\theta}(x)$$

$$P(y=0|x,\theta ) = 1- h_{\theta}(x)$$

* 将上式合成为：
$$P(y|x,\theta ) = h_{\theta}(x)^y(1-h_{\theta}(x))^{1-y}
$$其中，y的取值只能是0或者1。

* 得到了y的概率分布函数表达式，我们就可以用似然函数最大化来求解我们需要的模型系数θ。为了方便求解，这里我们用对数似然函数最大化，对数似然函数取反即为我们的损失函数J(θ)。似然函数的代数表达式为：
$$L(\theta) = \prod\limits_{i=1}^{m}(h_{\theta}(x^{(i)}))^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}
$$其中m为样本的个数。

* 对似然函数对数化取反的表达式，即损失函数表达式为：
$$J(\theta) = -lnL(\theta) = -\sum\limits_{i=1}^{m}(y^{(i)}log(h_{\theta}(x^{(i)}))+ (1-y^{(i)})log(1-h_{\theta}(x^{(i)})))$$

* 损失函数的矩阵形式：
$$J(\theta) = -Y^Tlogh_{\theta}(X) - (E-Y)^T log(E-h_{\theta}(X))
$$其中E为单位矩阵。

---

# 四、损失函数的优化
&#8195;  对于二元逻辑回归的损失函数极小化，有比较多的方法，最常见的有梯度下降法，坐标轴下降法，牛顿法等。这里将推导出梯度下降法中θ每次迭代的公式。

* 对于：
$$J(\theta) = -Y^T logh_{\theta}(X) - (E-Y)^T log(E-h_{\theta}(X))$$

* 我们用J(θ)对θ向量求导可得：
$$\frac{\partial}{\partial\theta}J(\theta) 
= X^T[\frac{1}{h_{\theta}(X)}\odot h_{\theta}(X)\odot (E-h_{\theta}(X))\odot (-Y)]+X^T[\frac{1}{E-h_{\theta}(X)}\odot h_{\theta}(X)\odot (E-h_{\theta}(X))\odot (E-Y)]$$

* 这一步我们用到了向量求导的链式法则，和下面三个基础求导公式的矩阵形式：
$$\frac{\partial}{\partial x}logx = 1/x$$

$$\frac{\partial}{\partial z}g(z) = g(z)(1-g(z))$$

$$\frac{\partial x\theta}{\partial \theta} =x$$
&#8195; 其中，g(z)为sigmoid函数。

* 我们对求导公式进行化简可得：
$$\frac{\partial}{\partial\theta}J(\theta) = X^T(h_{\theta}(X) - Y )$$

* 从而在梯度下降法中每一步向量θ的迭代公式如下：
$$\theta = \theta - \alpha X^T(h_{\theta}(X) - Y )
$$其中，α为梯度下降法的步长。

---

# 五、二元逻辑回归的正则化
&#8195;  逻辑回归也会面临过拟合问题，所以我们也要考虑正则化。常见的有L1正则化和L2正则化。逻辑回归的L1正则化的损失函数表达式如下，相比普通的逻辑回归损失函数，增加了L1的范数做作为惩罚，超参数α作为惩罚系数，调节惩罚项的大小。

## 1. L1正则化损失函数表达式如下：
$$J(\theta) = -Y^T \bullet logh_{\theta}(X) - (E-Y)^T\bullet log(E-h_{\theta}(X)) +\alpha ||\theta||_1
$$其中$||\theta||_1$为θ的L1范数。逻辑回归的L1正则化损失函数的优化方法常用的有坐标轴下降法和最小角回归法。

## 2. L2正则化损失函数表达式如下：
$$J( \theta ) = -Y^T \bullet logh_{\theta}(X) - (E-Y)^T\bullet log(E-h_{\theta}(X)) + \frac{1}{2}\alpha||\theta||_2^2
$$其中$|| \theta||_2$为θ的L2范数。逻辑回归的L2正则化损失函数的优化方法和普通的逻辑回归类似。

## 3. L1正则化和L2正则化的区别
* 1.两者引入的关于模型参数的先验知识不一样。L1正则化中w服从零均值拉普拉斯分布；L2正则化中w服从零均值正态分布。

* 2.L1偏向于使模型参数变得稀疏（但实际上并不那么容易），L2偏向于使模型每一个参数都很小，但是更加稠密，从而防止过拟合。

---

# 六、逻辑回归问题
## 1. 为什么逻辑回归的输入要先离散化/Onehot
* 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。

* 离散化后的特征对异常数据有很强的[*鲁棒性*](https://baike.baidu.com/item/%E9%B2%81%E6%A3%92%E6%80%A7)：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。
* 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。
* 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。
* 特征离散化后，模型会更稳定。比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。

## 2. 为什么逻辑回归不适合用MSE？
* 假设目标函数是MSE而不是交叉熵，即：
$$L=\frac{(y-\hat{y})^2}{2}，\frac{\partial L}{\partial w}=(\hat{y}-y)\sigma'(w\cdot x)x$$

* 这里sigmoid的导数项：
$$\sigma'(w\cdot x)=w\cdot x(1-w\cdot x)$$

* 根据$w$的初始化，导数值可能很小（想象一下sigmoid函数在输入较大时的梯度）而导致收敛变慢，而训练途中也可能因为该值过小而提早终止训练（参考梯度消失）。

* 另一方面，logloss的梯度如下，当模型输出概率偏离于真实概率时，梯度较大，加快训练速度，当拟合值接近于真实概率时训练速度变缓慢，没有MSE的问题。
$$g=\sum_{i=1}^N x_i(y_i-p(x_i))$$

---

# 七、逻辑回归和SVM的比较
### 共同点：
* 1. 都是有监督分类方法，判别模型（直接估计y=f(x)或p(y|x)）

* 2. 都是线性分类方法（指不用核函数的情况）

### 不同点：
* 1. loss不同。LR是交叉熵，SVM是Hinge loss，最大化函数间隔；

* 2. LR决策考虑所有样本点，SVM决策仅仅取决于支持向量；
* 3. LR受数据分布影响，尤其是样本不均衡时影响大，要先做平衡，SVM不直接依赖于分布；
* 4. LR可以产生概率，SVM不能；
* 5. LR不依赖样本之间的距离，SVM是基于距离的；
* 6. LR是经验风险最小化，需要另外加正则，SVM自带结构风险最小化不需要加正则项；
* 7. LR一般不用核函数，因为每两个点之间都要做内积，而SVM只需要计算样本和支持向量的内积，计算量更小；
* 8. 小数据集SVM比LR好，大数据SVM计算复杂，LR简单且能够在线并行训练，因此更多用LR。
