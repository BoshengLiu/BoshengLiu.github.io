# 损失函数
# 一、前言

&#8195;  机器学习模型需要有量化的评估指标来评估哪些模型的效果更好，本文将用比较通俗易懂的方式来讲解。首先需要了解线性和非线性，回归和分类。

## 1. 线性
* 简而言之就是"越...越..."，比如说"房子越大，价格越贵"等等...

## 2. 非线性
* 比如说"一个角度的余弦值/正弦值和角度呈非线性关系"，如下图所示：

![](https://upload-images.jianshu.io/upload_images/16911112-2ed5cc04325f7ad6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 3. 回归
* 回归的目的是为了预测，比如预测明天的天气温度，预测股票的走势等等… 回归之所以能预测是因为他通过历史数据，摸透了“套路”，然后通过这个套路来预测未来的结果。

## 4. 分类
* 例子：给你一堆猫和狗的图片，让你识别哪些是狗，哪些是猫。分类的结果是离散的、不连续的。

---

# 二、分类算法评价指标

&#8195;  有10个人，5男5女，男性为Positive，女性为Negative，预测正确为True，预测错误为False。则有：

|性别|男性(Positive)|女性(Negative)|
|:---:|:---:|:---:|
|男性(Positive)|男性(True)|男性(False)|
|女性(Negative)|女性(False)|女性(True)|

其中：

```
真正例(True Positive, TP)：被模型预测为正的正样本；
假正例(False Positive, FP)：被模型预测为正的负样本；
假负例(False Negative, FN)：被模型预测为负的正样本；
真负例(True Negative, TN)：被模型预测为负的负样本。
```

## 1. 准确率(Accuracy)
&#8195;  **准确率**是分类问题中最为原始的评价指标，准确率的定义是**预测正确的结果占总样本的百分比**，其公式如下：

$$Accuracy = \frac{TP+TN}{TP+TN+FP+FN}$$

&#8195;  但是，准确率评价算法有一个明显的弊端问题，就是在数据的类别不均衡时，特别是有极偏的数据存在的情况下，准确率这个评价指标是不能客观评价算法的优劣的。例如下面这个例子：
* 在测试集里，有100个样例，99个反例，只有1个正例。如果模型对任意一个样例都预测是反例，那么我的模型的准确率就为0.99，从数值上看是非常不错的，但事实上，这样的算法没有任何的预测能力，于是我们就应该考虑是不是评价指标出了问题，这时就需要使用其他的评价指标进行综合评判。

## 2. 精确率/差准率(Precision)

&#8195;  **精准率**（Precision）又叫**查准率**，它是针对预测结果而言的，它的含义是在**所有被预测为正的样本中实际为正的样本的概率**，意思就是在预测为正样本的结果中，我们有多少把握可以预测正确，其公式如下：

$$Precision = \frac{TP}{TP+FP}$$

&#8195;  精准率和准确率看上去有些类似，但是完全不同的两个概念。精准率代表对正样本结果中的预测准确程度，而准确率则代表整体的预测准确程度，既包括正样本，也包括负样本。

## 3. 召回率/查全率(Recall)

&#8195;  **召回率**（Recall）又叫**查全率**，它是针对原样本而言的，它的含义是**在实际为正的样本中被预测为正样本的概率**，其公式如下：
$$Recall = \frac{TP}{TP+FN}$$

&#8195;  引用维基百科中的图，可以查看二者关系：

![](https://upload-images.jianshu.io/upload_images/16911112-b711a723cdc13188.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

&#8195;  在不同的应用场景下，我们应该关注不同的点。在股票预测时，我们更关心精准率，即我们预测为升的股票里，升了的有多少。在预测病患时，我们更关注召回率，即真的患病的那些人里，预测错的情况应该越少越好。

&#8195;  精确率和召回率是一对此消彼长的度量。例如在推荐系统中，我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容，这样就漏掉了一些用户感兴趣的内容，召回率就低了；如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上，这样准确率就很低了。在实际工程中，我们往往需要结合两个指标的结果，去寻找一个平衡点，使综合性能最大化。

## 4. P-R曲线

&#8195;  **P-R曲线**（Precision Recall Curve）是描述精确率/召回率变化的曲线，P-R曲线定义如下：根据学习器的预测结果（一般为一个实值或概率）对测试样本进行排序，将最可能是“正例”的样本排在前面，最不可能是“正例”的排在后面，按此顺序逐个把样本作为“正例”进行预测，每次计算出当前的P值和R值，如下图所示：

![](https://upload-images.jianshu.io/upload_images/16911112-4d70966e63fb700b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

&#8195;  P-R曲线如何评估呢？若一个学习器A的P-R曲线被另一个学习器B的P-R曲线完全包住，则称：B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的，所以衍生出了“平衡点”（Break-Event Point，简称BEP），即当P=R时的取值，平衡点的取值越高，性能更优。

## 5. F1分数(F1-score)

&#8195;  **Precision**和**Recall**指标有时是此消彼长的，即精准率高了，召回率就下降，在一些场景下要兼顾精准率和召回率，最常见的方法就是F-Measure，又称**F-Score**。F-Measure是P和R的**加权调和平均**，即：
$$\frac{1}{F_{\beta}}=\frac{1}{1+\beta^{2}} \cdot\left(\frac{1}{P}+\frac{\beta^{2}}{R}\right)$$

$$F_{\beta}=\frac{\left(1+\beta^{2}\right) \times P \times R}{\left(\beta^{2} \times P\right)+R}$$

&#8195;  当β=1时，也就是常见的F1-Score，是P和R的调和平均，当F1较高时，模型的性能越好。

$$\frac{1}{F 1}=\frac{1}{2} \cdot\left(\frac{1}{P}+\frac{1}{R}\right)$$

$$F1=\frac{2 \times P \times R}{P+R} = \frac{2 \times TP}{样例总数+TP-TN}$$

## 6. ROC曲线

&#8195;  和上述评价指标相比，ROC曲线有个很好的特性：**当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变**。在实际的数据集中经常会出现**类别不平衡**(**Class** **Imbalance**)现象，即正负样本数目相差巨大，而且测试数据中的正负样本的分布也可能随着时间变化，ROC以及AUC可以很好的消除样本类别不平衡对指标结果产生的影响。

&#8195;  在正式介绍ROC之前，我们还要再介绍两个指标，这两个指标的选择使得ROC可以无视样本的不平衡。这两个指标分别是：**灵敏度(sensitivity)**和**特异度(specificity)**，也叫做**真正率(TPR)**和**假正率(FPR)**，具体公式如下。

* 真正率(True Positive Rate , TPR)，又称**灵敏度**：
$$TPR = \frac{正样本预测正确数}{正样本总数} = \frac{TP}{TP+FN}$$

* 假负率(False Negative Rate , FNR) ：
$$FNR = \frac{正样本预测错误数}{正样本总数} = \frac{FN}{TP+FN}$$

* 假正率(False Positive Rate , FPR) ：
$$FPR = \frac{负样本预测错误数}{负样本总数} = \frac{FP}{TN+FP}$$

* 真负率(True Negative Rate , TNR)，又称特异度：
$$TNR = \frac{负样本预测正确数}{负样本总数} = \frac{TN}{TN+FP}$$

&#8195;  细分析上述公式，我们可以可看出，灵敏度（真正率）TPR是正样本的召回率，特异度（真负率）TNR是负样本的召回率，而假负率*FNR=1−TPR*、假正率*FPR=1−TNR*，上述四个量都是针对单一类别的预测结果而言的，所以对整体样本是否均衡并不敏感。举个例子：

* 假设总样本中，90%是正样本，10%是负样本。在这种情况下我们如果使用准确率进行评价是不科学的，但是用TPR和TNR却是可以的，因为TPR只关注90%正样本中有多少是被预测正确的，而与那10%负样本毫无关系，同理，FPR只关注10%负样本中有多少是被预测错误的，也与那90%正样本毫无关系。这样就避免了样本不平衡的问题。

&#8195;  **ROC**(**Receiver** **Operating** **Characteristic**)**曲线**，又称**接受者操作特征曲线**。该曲线最早应用于雷达信号检测领域，用于区分信号与噪声。后来人们将其用于评价模型的预测能力。ROC曲线中的主要两个指标就是**真正率TPR**和**假正率FPR**，上面已经解释了这么选择的好处所在。其中横坐标为假正率(FPR)，纵坐标为真正率(TPR)，下面就是一个标准的ROC曲线图。

![](https://upload-images.jianshu.io/upload_images/16911112-34619a08bb736422.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

&#8195;  与前面的P-R曲线类似，ROC曲线也是通过遍历所有阈值来绘制整条曲线的。如果我们不断的遍历所有阈值，预测的正样本和负样本是在不断变化的，相应的在ROC曲线图中也会沿着曲线滑动。我们看到改变阈值只是不断地改变预测的正负样本数，即TPR和FPR，但是曲线本身并没有改变。这是有道理的，阈值并不会改变模型的性能。

![](https://upload-images.jianshu.io/upload_images/16911112-6493d5c4cc9e2aeb.gif?imageMogr2/auto-orient/strip)

&#8195;  那么如何判断一个模型的ROC曲线是好的呢？这个还是要回归到我们的目的：FPR表示模型对于负样本误判的程度，而TPR表示模型对正样本召回的程度。我们所希望的当然是：负样本误判的越少越好，正样本召回的越多越好。所以总结一下就是TPR越高，同时FPR越低（即ROC曲线越陡），那么模型的性能就越好。参考如下动态图进行理解。

![](https://upload-images.jianshu.io/upload_images/16911112-6e4794706d90a4c1.gif?imageMogr2/auto-orient/strip)

&#8195;  即：进行模型的性能比较时，与PR曲线类似，若一个模型A的ROC曲线被另一个模型B的ROC曲线完全包住，则称B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。

&#8195;  前面已经对ROC曲线为什么可以无视样本不平衡做了解释，下面我们用动态图的形式再次展示一下它是如何工作的。我们发现：无论红蓝色样本比例如何改变，ROC曲线都没有影响。

![](https://upload-images.jianshu.io/upload_images/16911112-7b48e4b8cb83b07e.gif?imageMogr2/auto-orient/strip)

## 7. AUC曲线

&#8195;  **AUC**(**Area** **Under** **Curve**)又称为**曲线下面积**，是处于**ROC** **Curve**下方的那部分面积的大小。上文中我们已经提到，对于ROC曲线下方面积越大表明模型性能越好，于是AUC就是由此产生的评价指标。通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的Performance。如果模型是完美的，那么它的AUC=1，证明所有正例排在了负例的前面，如果模型是个简单的二类随机猜测模型，那么它的AUC=0.5，如果一个模型好于另一个，则它的曲线下方面积相对较大，对应的AUC值也会较大。

&#8195;  AUC对所有可能的分类阈值的效果进行综合衡量。首先AUC值是一个**概率值**，可以理解为随机挑选一个正样本以及一个负样本，分类器判定正样本分值高于负样本分值的概率就是AUC值。简言之，AUC值越大，当前的分类算法越有可能将正样本分值高于负样本分值，即能够更好的分类。

## 8. 混淆矩阵

&#8195;  **混淆矩阵**(Confusion Matrix)又被称为**错误矩阵**，通过它可以直观地观察到算法的效果。它的每一列是样本的预测分类，每一行是样本的真实分类（反过来也可以），顾名思义，它反映了分类结果的混淆程度。混淆矩阵i行j列的原始是原本是类别i却被分为类别j的样本个数，计算完之后还可以对之进行可视化：

![](https://upload-images.jianshu.io/upload_images/16911112-feae0f3426126c9a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

&#8195;  对于多分类问题，或者在二分类问题中，我们有时候会有多组混淆矩阵，例如：多次训练或者在多个数据集上训练的结果，那么估算全局性能的方法有两种，分为**宏平均**(macro-average)和**微平均**(micro-average)。简单理解，宏平均就是先算出每个混淆矩阵的P值和R值，然后取得平均P值macro-P和平均R值macro-R，再算出Fβ或F1，而微平均则是计算出混淆矩阵的平均TP、FP、TN、FN，接着进行计算P、R，进而求出Fβ或F1。其它分类指标同理，均可以通过宏平均/微平均计算得出。

$$\operatorname{macro}P=\frac{1}{n}\sum_{i=1}^{n}P_{i}\tag{1}$$

$$\operatorname{macro}R=\frac{1}{n}\sum_{i=1}^{n}R_{i}\tag{2}$$

$$\operatorname{macro}F1=\frac{2\times\operatorname{macro}P\times\operatorname{macro}R}{\operatorname{macro} P+\operatorname{macro}R}\tag{3}$$

$$\operatorname{micro}P=\frac{\overline{TP}}{\overline{TP}+\overline{FP}}\tag{4}$$

$$\operatorname{micro}R=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}\tag{5}$$

$$\operatorname{micro}F1=\frac{2\times\operatorname{micro}P\times\operatorname{micro}R}{\operatorname{micro} P+\operatorname{micro}R}\tag{6}$$

&#8195;  需要注意的是，在多分类任务场景中，如果非要用一个综合考量的metric的话，宏平均会比微平均更好一些，因为宏平均受稀有类别影响更大。宏平均平等对待每一个类别，所以它的值主要受到稀有类别的影响，而微平均平等考虑数据集中的每一个样本，所以它的值受到常见类别的影响比较大。

---

# 三、回归算法评价指标
* 注：$y_{i}$为原始数据，$\hat{y}$为预测值，$\bar{y}$为均值

## 1. 均方误差(MSE)

&#8195;  MSE是真实值与预测值的差值的平方然后求和平均，通过平方的形式便于求导，所以常被用作线性回归的损失函数(即L1)。
$$MSE=\frac{1}{m}\sum_{i=1}^{m}(y_{i}-\bar{y})^{2}$$

## 2. 均方根误差(RMSE)
&#8195;  RMSE是衡量观测值与真实值之间的偏差，常用来作为机器学习模型预测结果衡量的标准。
$$RMSE=\sqrt{\frac{1}{m}\sum_{i=1}^{m}(y_{i}-\hat{y})^{2}}$$

## 3. 平均绝对误差(MAE)
&#8195;  绝对误差的平均值，可以更好地反映预测值误差的实际情况
$$MAE=\frac{1}{m}\sum_{i=1}^{m}\vert(y_{i}-\hat{y})\vert$$

## 4. 误差平方和(SSE)
$$SSE=\sum_{i=1}^{m}(y_{i}-\hat{y})^{2}$$

* 同样的数据集的情况下，SSE越小，误差越小，模型效果越好。 
* SSE数值大小本身没有意义，随着样本增加，SSE必然增加，也就是说，不同的数据集的情况下，SSE比较没有意义。

## 5. 决定系数/拟合系数$R^{2}$
$$RSS=\sum_{i=1}^{m}(y_{i}-\hat{y})^{2}$$

$$TSS=\sum_{i=1}^{m}(y_{i}-\bar{y})^{2}$$

$$R^{2}=1-\frac{RSS}{TSS}$$
其中，RSS为预测数据和原始数据的误差，TSS为原始数据的离散程度。**决定系数**是通过数据的变化来表征一个拟合的好坏，理论上取值范围(-∞,1]，正常取值范围为[0,1]。

* 越接近1，表明方程的变量对y的解释能力越强，这个模型对数据拟合的也较好；  
* 越接近0，表明模型拟合的越差；   
* 经验值：>0.4，拟合效果好。

缺点：数据集的样本越大，R²越大，因此，不同数据集的模型结果比较会有一定的误差

## 6. 校正决定系数$R^{2}_{adjusted}$
$$R^{2}_{adjusted}=1-\frac{(1-R^{2})(n-1)}{n-p-1}$$
其中，n为样本数量，p为特征数量，和$R^{2}$系数相比，消除了样本数量和特征数量的影响。
