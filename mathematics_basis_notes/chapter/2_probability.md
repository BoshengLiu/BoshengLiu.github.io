# 概率论与随机过程
# 一、概率与分布
## 1. 条件概率与独立事件
1. 条件概率：已知$a$事件发生的条件下$B$发生的概率，记作$P(B|A)$，它等于事件$AB$的概率相对于事件$A$的概率，即：$P(B|A)$。其中必须有$P(A)>0$。

2. 条件概率分布的链式法则：对于$n$个随机变量$X_1,X_2,...,X_n$，有：
$$P(X_1,X_2,...,X_n)=P(X_1)\prod_{i=2}^{n}P(X_i|X_1,...,X_{i-1})$$

3. 两个随机变量$X,Y$相互独立的数学描述：$P(X,Y)=P(X)P(Y)$。记作：$X\bot Y$。

4. 两个随机变量$X,Y$关于随机变量$Z$条件独立的数学描述：$P(X,Y|Z)=P(X|Z)P(Y|Z)$。记作：$X\bot Y|Z$。


## 2. 联合概率分布
1. 定义$X$和$Y$的联合分布为：$P(a,b)=P\lbrace X\leq a,Y\leq b\rbrace,\quad -\infty<a,b<+\infty$。
    * $X$的分布可以从联合分布中得到：
$$P_X(a)=P\lbrace X\leq a\rbrace=P\lbrace X\leq a,Y\leq \infty\rbrace=P(a,\infty),\quad -\infty<a<+\infty$$
    * $Y$的分布可以从联合分布中得到：
$$P_Y(b)=P\lbrace Y\leq b\rbrace=P\lbrace X\leq\infty,Y\leq b\rbrace=P(\infty,b),\quad -\infty<b<+\infty$$

2. 当$X$和$Y$都是离散随机变量时， 定义$X$和$Y$的联合概率质量函数为：$p(x,y)=P\lbrace X=x,Y=y\rbrace$，则$X$和$Y$的概率质量函数分布为：
$$p_X(x)=\sum_{y}p(x,y),\quad p_{Y}(y)=\sum_{x}p(x,y)$$

3. 当$X$和$Y$联合地连续时，即存在函数$p(x,y)$，使得对于所有的实数集合$A$和$B$满足：
$$P\lbrace X\in A,Y\in B\rbrace=\int_B\int_A p(x,y)dxdy$$
则函数$p(x,y)$称为$X$和$Y$的概率密度函数。
    * 联合分布为：
$$P(a,b)=P\lbrace X\leq a,Y\leq b \rbrace=\int_{-\infty}^{a}\int_{-\infty}^{b}p(x,y)dxdy$$
    * $X$和$Y$的分布函数以及概率密度函数分别为：
$$P_X(a)=\int_{-\infty}^{a}\int_{-\infty}^{\infty}p(x,y)dxdy=\int_{-\infty}^{a}p_X(x)dx\\[2ex]
P_Y(b)=\int_{-\infty}^{\infty}\int_{-\infty}^{b}p(x,y)dxdy=\int_{-\infty}^{b}p_Y(y)dy\\[2ex]
p_X(x)=\int_{-\infty}^{\infty}p(x,y)dy\quad\quad p_Y(y)=\int_{-\infty}^{\infty}p(x,y)dx$$

---

# 二、期望和方差
## 1. 期望
1. 期望描述了随机变量的平均情况，衡量了随机变量$X$的均值。它是概率分布的泛函（函数的函数）。
    * 离散型随机变量$X$的期望：$E(X)=\sum_{i=1}^{\infty}x_ip_i$。若右侧级数不收敛，则期望不存在。
    
    * 连续性随机变量$X$的期望：$E(X)=\int_{-\infty}^{\infty}xp(x)dx$。若右侧极限不收敛，则期望不存在。

2. 定理：对于随机变量$X$，设$Y=g(X)$也为随机变量，$g(\cdot)$是连续函数。
    * 若$X$为离散型随机变量，若$Y$的期望存在，则：
$$E[Y]=E[g(X)]=\sum_{i=1}^{\infty}g(x_i)p_i$$
也记做：$$E_{X\sim P(X)}[g(X)]=\sum_xg(x)p(x)$$
    * 若$X$为连续型随机变量，若$Y$的期望存在，则：
$$E[Y]=E[g(X)]=\int_{-\infty}^{\infty}g(x)p(x)dx$$
也记做：$$E_{X\sim P(X)}[g(X)]=\int g(x)p(x)dx$$
该定理的意义在于：当求$E(Y)$时，不必计算出$Y$的分布，只需要利用$X$的分布即可。
该定理可以推广至两个或两个以上随机变量的情况。对于随机变量$X,Y$，假设$Z=g(X,Y)$也是随机变量，$g(\cdot)$为连续函数，则有：
$$E[Z]=E[g(X,Y)]=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x,y)p(x,y)dxdy$$
也记做：
$$E_{X,Y\sim P(X,Y)}|g(X,Y)|\int g(x,y)p(x,y)dxdy$$

3. 期望性质：
    * 常数的期望就是常数本身。
    
    * 对常数$C$有：$E[CX]=CE[X]$。
    * 对两个随机变量$X,Y$，有：$E|X+Y|=E[X]+E[Y]$。该结论可以推广到任意有限个随机变量之和的情况。
    * 对两个相互独立的随机变量$X,Y$，有：$E|XY|=E[X]E[Y]$。该结论可以推广到任意有限个相互独立的随机变量之积的情况。

## 2. 方差
1. 对随机变量$X$，若$E[(X-E[X])^2]$存在，则称它为$X$的方差，记作$Var[X]$。 $X$的标准差为方差的开平方。即：
$$Var[X]=E[(X-E[X])^2]\\[2ex] \sigma = \sqrt{Var[X]}$$
    * 方差度量了随机变量$X$与期望值偏离的程度，衡量了$X$取值分散程度的一个尺度。
    
    * 由于绝对值$|X-E[X]|$带有绝对值，不方便运算，因此采用平方来计算。
    * 又因为$|X-E[X]|^2$是一个随机变量，因此对它取期望，即得$X$与期望值偏离的均值。

2. 根据定义可知：
$$Var[X]=E[(X-E[X])^2]=E[X^2]-(E[X])^2\\[2ex]
Var[f(X)]=E[(f(X)-E[f(X)])^2]$$

3. 对于一个期望为$\mu$， 方差为$\sigma,\sigma\neq 0$的随机变量$X$，随机变量$X^*=\frac{X-\mu}{\sigma}$的数学期望为0，方差为1。 称$X^*$为$X$的标准化变量。

4. 方差的性质：
    * 常数的方差恒为0。
    
    * 对常数$C$，有$Var[CX]=C^2vAR[X]$。
    * 对两个随机变量$X,Y$，有： 
$$Var[X+Y]=Var[X]+Var[Y]+2E[(X-E[X])(Y-E[Y])]$$
当$X$和$Y$相互独立时，有：
$$Var[X+Y]=Var[X]+Var[Y]$$
这可以推广至任意有限多个相互独立的随机变量之和的情况。

## 3. 协方差与相关系数
1. 对于二维随机变量$X,Y$，可以讨论描述$X$与$Y$之间相互关系的数字特征。
    * 定义$E[(X-E[X])(Y-E[Y])]$为随机变量$X$与$Y$的协方差，记作  
$$Cov[X,Y]=E[(X-E[X])(Y-E[Y])]$$
    
    * 定义$\rho_{XY}=\frac{Cov[X,Y]}{\sqrt{Var[X]}\sqrt{Var[Y]}}$为随机变量$X$与$Y$的相关系数，它是协方差的归一化。

2. 由定义可知：
$$Cov[X,Y]=Cov[Y,X]\\[2ex]
Cov[X,X]=Var[X]\\[2ex]
Var[X+Y]=Var[X]+Var[Y]+2Cov[X,Y]$$

3. 协方差的性质：
    * $Cov[aX,bY]=abCov[X,Y]，a,b$为常数。
    
    * $Cov[X_1+X_2,Y]=Cov[X_1,Y]+Cov[X_2,Y]$
    
    * $Cov[f(X),g(X)]=E[(f(X)-E[f(X)])(g(Y)-E[g(Y)])]$
    
    * $\rho[f(X),g(Y)]=\frac{Cov[f(X),g(Y)]}{\sqrt{Var[f(X)]}\sqrt{Var[g(Y)]}}$

4. 协方差的物理意义：
    * 协方差的绝对值越大，说明两个随机变量都远离它们的均值。

    * 协方差如果为正，则说明两个随机变量同时趋向于取较大的值或者同时趋向于取较小的值；如果为负，则说明一个随变量趋向于取较大的值，另一个随机变量趋向于取较小的值。

    * 两个随机变量的独立性可以导出协方差为零。但是两个随机变量的协方差为零无法导出独立性。因为独立性也包括：没有非线性关系。有可能两个随机变量是非独立的，但是协方差为零。如：假设随机变量$X\sim U[-1,1]$。定义随机变量$S$的概率分布函数为：
$$P(S=1)=\frac{1}{2}P(S=-1)=\frac{1}{1}$$
定义随机变量$Y=SX$，则随机变量$X,Y$是非独立的，但是有：$Cov[X,Y]=0$。

5. 相关系数的物理意义：考虑以随机变量$X$的线性函数$a+bX$来近似表示$Y$。以均方误差
$$e=E[(Y-(a+bX))^2]=E[Y^2]+b^2E[X^2]+a^2-2bE[XY]+2abE[X]-2aE[Y]$$
来衡量以$a+bX$近似表达$Y$的好坏程度。$e$越小表示近似程度越高。
为求得最好的近似，则对$a,b$分别取偏导数，得到：
$$a_0=E[Y]-b_0E[X]=E[Y]-E[X]\frac{Cov[X,Y]}{Var[X]}\\[2ex]
b_0=\frac{Cov[X,Y]}{Var[X]}\\[2ex]
min(e)=E[(Y-(a_0+b_0X))^2]=(1-\rho^2_{XY})Var[Y]$$
因此有以下定理：
    * $|\rho_{XY}|\leq 1(|·|是绝对值)$。
    
    * $|\rho_{XY}|=1$的充要条件是：存在常数$a,b$使得$P\lbrace Y=a+bX \rbrace=1$。

6. 当$|\rho_{XY}|$较大时，$e$较小，意味着随机变量$X和Y$联系较紧密。于是$\rho_{XY}$是一个表征$X、Y$之间线性关系紧密程度的量。

7. 当$\rho_{XY}=0$时，称$x和y$不相关。
    * 不相关是就线性关系来讲的，而相互独立是一般关系而言的。
    
    * 相互独立一定不相关；不相关则未必独立。

## 4. 协方差矩阵
1. 设$X和Y$是随机变量。
    * 若$E[X^k],k=1,2,...,$存在，则称它为$X$的$k$阶原点矩，简称$k$阶矩。
    
    * 若$E[(X-E[X])^k],k=1,2,...$存在，则称它为$X$的$k$阶中心矩。
    * 若$E[X^kY^l],k,l=1,2,...$存在，则称它为$X$和$Y$的$k+l$阶混合矩。
    * 若$E[(X-E[X])^k] (Y-E[Y])^l,k,l=1,2,...$存在，则称它为$X$和$Y$的$k+l$阶混合中心矩。
因此：期望是一阶原点矩，方差是二阶中心矩，协方差是二阶混合中心矩。

2. 协方差矩阵：
    * 二维随机变量$(X_1,X_2)$有四个二阶中心矩（假设他们都存在），记作：
$$c_{11}=E[(X_1-E[X_1])^2]\\[2ex]
c_{12}=E[(X_1-E[X_1])(X_2-E[X_2])]\\[2ex]
c_{31}=E[(X_2-E[X_2])(X_1-E[X_1])]\\[2ex]
C_{22}=E[(X_2-E[X_2])^2]$$
称矩阵
$$C=\begin{bmatrix} c_{11} & c_{12} \\ c_{21} & c_{22} \\ \end{bmatrix}$$
为随机变量$(X_1,X_2)$的协方差矩阵。

    * 设$n$维随机变量$(X_1,X_2,...,X_n)$的二阶混合中心矩$c_{i,j}=Cov[X_i,X_j]=E[(X_i-E[X_i])(X_j-E[X_j])]$都存在，则称矩阵
$$C=\begin{bmatrix}
c_{11} & c_{12} & \cdots & c_{1n}\\ 
c_{21} & c_{22} & \cdots & c_{2n}\\ 
\vdots & \vdots & \ddots & \vdots\\ 
c_{n1} & c_{n2} & \cdots & c_{nn}\\ 
\end{bmatrix}$$
为$n$维随机变量$(X_1,X_2,...,X_n)$的协方差矩阵。由于$c_{ij}=c_{ji},i\neq j,i,j=1,2,...,n$因此协方差矩阵是个对称阵。

3. 通常$n$维随机变量的分布是不知道的，或者太复杂以致数学上不容易处理。因此实际中协方差矩阵非常重要。

---

# 三、常见概率分布
## 前言
&#8195;  本文将介绍一下机器学习中用到的概率分布。这些分布不仅在机器学习中有广泛使用，而且在各个领域都有用到。

## 1. 二项分布
1. 假设试验只有两种结果：成功的概率为$\phi$，失败的概率$1-\phi$。 则二项分布描述了：独立重复地进行$n$次试验中，成功$x$次的概率。
    * 概率质量核函数：
$$p(X=x)=\frac{n!}{x!(n-x)!}\phi^{x}(1-\phi)^{n-x},x\in \lbrace 0,1,...,n \rbrace$$

    * 期望：$E[X]=n\phi。方差：Var[X]=n\phi(1-\phi)$。

## 2. 多项分布
&#8195;  二项分布用于建模丢硬币这样的结果有两种可能的事件，对于结果有K中可能的事件，我们用多项式分布建模。假设$x= (x_1,…,x_k)$是一个随机矢量，$x_j$表示第j个可能出现的次数，我们有$\sum_ {i=1}^{K}x_{i} = n$，则x具有的分布是:

$$\mathrm {Mu}(\mathbf{x}| n, \mathbf{\theta}) = \binom{n}{x_{1},\ldots ,x_{k}}\prod_{j=1}^{K}\theta_{j}^{x_{j}}$$

&#8195;  $θ_j$代表第j个可能出现的概率，$\binom{n}{x_{1},\ldots,x_{K}} = \frac{n!}{x_{1}!x_{2}!\ldots x_{K}!}$是多项系数，这个多项式系数是把$n= \sum_{k=1}^{K}x_{k}$分成$x_ {1},\ldots ,x_{K}$的所有可能的分法。


## 3. 泊松分布
&#8195;  随机变量X服从泊松分布，则其概率密度分布可以表示为：

$$\mathrm{Poi}(x|\lambda) = e^{-\lambda} \frac{\lambda^{x}}{x!}$$

&#8195;  其中$e^{−λ}$是归一化常量，用于保证分布的和是1。泊松分布经常用于对较少发生的事情建模，比如从沙子里找金子。


## 4. 经验分布
&#8195;  给定一个数据集$D={x_1,…,x_N}$，我们定义一个经验分布（也叫经验度量）:
$$P_{\mathrm{emp}}(A) = \frac{1}{N} \sum_{i=1}^{N}\delta_{x_{i}} (A)$$

&#8195;  其中$δ_{x}(A)$是狄拉克度量。
$$\delta_{x}(A) = \begin{cases} 0 & x\notin A \\ 1 & x\in A \end{cases}$$

&#8195;  通常意义上我们为每一个样本分配一个权重：
$$p(x) = \sum_{i=1}^{N}w_{i}\delta_{x_{i}}(x)$$

&#8195;  其中$w_{i}$是权重，满足$0≤w_{i}≤1$，且$\sum_{i=1}^{N}w_{i} = 1$。我们可以把这个想象成一个直方图，在每一个$x_i$处都有一个高为$w_{i}$的直方柱。


## 5. 高斯分布
&#8195;  高斯分布是机器学习和统计学中使用最广泛的分布，其密度函数是：
$$\mathcal{N}(x|\mu,\sigma^{2}) = \frac{1}{\sqrt{2\pi \sigma^{2}}}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}$$

&#8195;  这里μ=E(X)是均值，$σ^2$是方差。$\sqrt{2\pi\sigma^{2}}$是归一化因子，用来保证密度函数积分为1。

&#8195;  我们用$X∼N(μ,σ^2)$来表示X服从高斯分布，其密度函数为$p(x|\mu, \sigma^{2})$，如果X∼N(0,1)，我们说X服从正态分布。通常，我们用方差的导数表示高斯分布的精度$λ=1/σ^{2}$，一个高精度的高斯密度函数指的是一个方差较小的高斯函数。这一意味着，随机变量的值大都集中于均值μ周围。考虑在x=μ出的概率，其值为$\mathcal{N}(\mu|\mu,\sigma^{2}) = \frac{1}{2\pi \sigma^{2}}$，所以如果$\sigma < \sqrt{2\pi}$，我们有p(x)>1，即出现了概率大于1的现象。

&#8195;  高斯函数的累积分布函数(CDF)定义为：
$$\Phi(x|\mu,\sigma^{2})= \int_{-\infty}^{x} \mathcal{N}(z|\mu,\sigma^{2}) \mathrm{d}z$$

&#8195;  这个CDF没有闭式解析解，但是很多软件包都可以用来计算该CDF。高斯函数之所以在统计学中有广泛应用，基于以下原因：
* 1.高斯分布只用两个参数就可以解释：均值和方差。
* 2.中心极限定理告诉我们：独立随机变量的和近似于高斯分布。所以我们用高斯变量来建模噪声。
* 3.使用高斯分布用最少的假设条件就可以构建满足某种均值和方差的分布。
* 4.高斯分布的数学形式简单，易于实现。

### 高斯分布的退化
&#8195;  当我们令密度函数中的$σ^2$趋近零的时候，高斯分布的精度会越来越高，意味着高斯分布越来越窄，窄到只在x=μ处有值，即
$$\lim_{\sigma^{2}\to 0} \mathcal{N}(x|\mu,\sigma^{2}) = \delta(x-\mu)$$

&#8195;  其中δ(x)是狄拉克函数：
$$
\delta(x) =
\begin{cases}
1 & x = 0 \\
0 & x\neq 0
\end{cases}
$$

且有：
$${eq:14}\int_{-\infty}^{\infty} \delta(x) \mathrm{d}x = 1$$

&#8195;  这个狄拉克函数的最大作用是采样，可以用这个函数从一个和中选出一项，比如：
$${eq:15}\int_{-\infty}^{\infty}f(x)\delta(x-\mu)dx = f(\mu)$$

&#8195;  高斯函数的一个缺点是对异类(outlier)特别敏感，这是因为高斯函数取log之后只随着$(x−μ)^2$衰减，x−μ是x到高斯函数中心的距离。一个更鲁棒的分布是学生分布。


## 6. 学生t分布
&#8195;  学生分布密度函数：
$$\mathcal{T}(x| \mu,\sigma^{2},\nu) \propto \bigg[ 1 + \frac{1}{\nu}\big(\frac{x-\mu}{\sigma}\big)^{2} \bigg]^{-\frac{\nu + 1}{2}}$$

&#8195;  其中μ是均值，$ \sigma^{2}>0$是缩放因子，ν>0是自由度。稍后我们会发现学生分布的方差是$\frac{ \nu \sigma^{2}}{ \nu-2}$。

![](https://upload-images.jianshu.io/upload_images/16911112-7e23155c3ca51d73.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

&#8195;  我们从图1可以看出高斯分布对异类更敏感，而学生分布则要稳定的多。对于学生分布$\nu =1$时，我们也称其为柯西分布或者洛伦兹分布。这个分布的尾巴很重以至于求均值计算积分的时候根本不收敛。

&#8195;  为了保证方差有限，我们要求是$\nu >2$,$\nu =4$比较常见的一个参数。当$\nu \ggg 5$的时候，学生分布近似于高斯分布，逐渐失去其鲁棒性。


## 7. 拉普拉斯分布
&#8195;  拉普拉斯分布也是具有重尾的分布，这个分布也叫作双边指数分布，从其定义式上就可以看出为什么这么称呼：
$$\mathrm{Lap}(x| \mu ,b) = \frac{1}{2b} \mathrm{exp} \bigg( -\frac{|x-\mu|}{b} \bigg)$$

&#8195;  这里μ是位置参数,b是一个缩放参数。拉普拉斯分布的均值是μ，方差是$2b^2$。这个分布对于异类也是不敏感的，在0附近聚集了更多的概率密度。使用这个分布我们可以在模型中允许更多的稀疏性(sparsity)。


## 8. 伽马分布
&#8195;  伽马分布是一个非常灵活的分布。伽马分布由两个参数决定形状参数a，和速率参数b:
$$\mathrm{Ga}(T| \mathrm{shape} = a, \mathrm{rate} = b) = \frac{b^{a}}{\Gamma(a)}T^{a-1}e^{-Tb}$$

&#8195;  其中Γ(a)是伽马函数：
$$\Gamma(x) = \int_{0}^{\infty} u^{x-1} e^{-u} du$$

&#8195;  伽马分布的主要参数是：
$$\mathrm{mean} = \frac{a}{b}, \mathrm{mode} = \frac{a-1}{b}, \mathrm{var} = \frac{a}{b^{2}}$$

&#8195;  有几个分布是伽马分布的特例：
* 1. 指数分布，定义为：
$$\mathrm{Expon}(x|\lambda) \mathrm{Ga}(x|1,\lambda)$$
其中λ是速率参数，指的是连续发生事件的平均发生频率。

* 2. Erlang分布。这个分布和伽马分布相同，除了a是一个整数。我们令a=2就是Erlang分布，即
$$\mathrm{Erlang}(x|\lambda) = \mathrm{Ga}(x|2,\lambda)$$
其中λ是速率参数，指的是连续发生事件的平均发生频率。

* 3. $x^2$分布。这个分布定义为：
$$\chi^{2}(x|\nu) = \mathrm{Ga}(x| \frac{\nu}{2},\frac{1}{2} )$$
这个分布是高斯随机变量平方和的分布，即如果$ Z_{i}\sim \mathcal{N}(0,1)$，$ S = \sum_{i=1}^{\nu}Z_{i}^{2}$，那么$S\sim \chi_{\nu}^{2}$。

## 9. 贝塔分布
&#8195;  贝塔分布定义域在[0,1]上，定义式为：
$$\mathrm{Beta}(x|a,b) = \frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1}$$

&#8195;  这里B(p,q)是贝塔函数：
$$B(a,b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$$

&#8195;  为确保B(a,b)存在，a,b必须是大于零的数。当a=b=1时，贝塔分布退化为均匀分布。贝塔分布的均值是$\frac{a}{a+b}$，方差为$\frac{ab}{(a+b)^{2}(a+b+1)}$。

## 10. Pareto分布
&#8195;  **帕累托分布**(Pareto distribution)是以意大利经济学家维尔弗雷多·帕累托命名的。 是从大量真实世界的现象中发现的幂定律)分布。这个分布在经济学以外，也被称为**布拉德福分布**。

&#8195;  在帕累托分布中，如果*X*是一个随机变量，则*X*的概率分布如下面的公式所示：
$$P(X>x)=\left(\frac{x}{x_{min}} \right) ^2$$

&#8195;  其中x是任何一个大于$x_{min}$的数，$x_{min}$是X最小的可能值（正数），k是为正的参数。

&#8195;  帕累托分布曲线族是由两个数量参数化的：$x_{min}$和k。分布密度则为:
$$f(n)= \begin{cases} 
0,  & \text {if} x < x_{min}\\[2ex]
\frac{k x_{min}^k}{x^{k+1}}, & \text{if}x > x_{min}
\end{cases}$$
帕累托分布属于连续概率分布。 

1. 如果$k \leq 1$, 期望值为无穷大，且随机变量的标准差为$\frac {x_{min}}{k1} \sqrt{ \frac{k}{k-2}}$。一个遵守帕累托分布的随机变量的期望值为 ：$ \frac{x_{min}k}{k-1}$
2. 如果$ k \leq 2$， 标准差不存在。

## 11. 机器学习中概率分布
&#8195;  讨论概率分布的一个重要原因是，现实生活中有很多数据可以使用这些模型来模拟。对于给定的一个数据集合${x_1, … , x_n}$我们希望这个数据集合来自于某个随机变量X，并且这个随机变量具有概率分布P(X)。找到P(X)的过程叫做密度估计（density estimation）。需要强调的是密度估计问题是一个病态问题，因为世界上的概率密度函数不计其数，能够给出观测集合${x_1, … , x_n}$的概率密度函数也是如此之多。任何一个在${x_1, … , x_n}$处非零的密度函数P(X)都可能是候选。选择一个合适的P(X)是模型选择问题，在机器学习领域经常遇见。

&#8195;  之前我们也看到了很多分布，把这些分布用到密度估计过程中，我们需要确定相对于这些模型的一些重要参数，比如期望，比如方差，再比如一些超参数。这些量值的确定亦有不同的方法，从频率方面（frequentist）着手的话，我们根据某个准则进行优化，为这些参数选择特定的值，常见的准则是最大似然准则。从贝叶斯方面着手的话，我们引入这些参数的一些先验估计（prior distributions）,然后使用贝叶斯定理根据给定的观测数据计算关于这些参数的后验估计。

---

# 四、先验分布和后验分布
1. 在贝叶斯学派中，`先验分布+数据（似然）= 后验分布`。

2. 例如：假设需要识别一大箱苹果中的好苹果、坏苹果的概率。
    * 根据你对苹果好、坏的认知，给出先验分布为：50个好苹果和50个坏苹果。

    * 现在你拿出10个苹果，发现有：8个好苹果，2个坏苹果。       
根据数据，你得到后验分布为：58个好苹果，52个坏苹果

    * 再拿出10个苹果，发现有：9个好苹果，1个坏苹果。     
根据数据，你得到后验分布为：67个好苹果，53个坏苹果

    * 这样不断重复下去，不断更新后验分布。当一箱苹果清点完毕，则得到了最终的后验分布。  
在这里：
    * 如果不使用先验分布，仅仅清点这箱苹果中的好坏，则得到的分布只能代表这一箱苹果。
    * 采用了先验分布之后得到的分布，可以认为是所有箱子里的苹果的分布。
    * 当采用先验分布时：给出的好、坏苹果的个数（也就是频数）越大，则先验分布越占主导地位。







---

# 五、大数定律及中心极限定理
## 1. 切比雪夫不等式





## 2. 大数定理






## 3. 中心极限定理







---

# 六、信息论

