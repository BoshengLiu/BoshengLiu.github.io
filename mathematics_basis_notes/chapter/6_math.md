# 数学符号
# 1. 凸函数/凹函数
* 凸函数是一个定义在某个向量空间的凸子集 $C$ 上的实值函数 $f$，而且对于凸子集 $C$ 中任意两个向量，有 $f((x_1+x_2)/2)<=(f(x_1)+f(x_2))/2$ 成立。

* 凸函数的性质有：

    * 1. 若 $f$ 为定义在凸集$S$上的凸函数，则对任意实数 $β≥0$，函数 $β_f$ 也是定义在 $S$ 上的凸函数；
    
    * 2. 若 $f_1$ 和 $f_2$ 为定义在凸集 $S$ 上的两个凸函数，则其和 $f=f_1+f_2$ 仍为定义在 $S$ 上的凸函数；
    
    * 3. 若 $f_i(i=1，2，…，m)$ 为定义在凸集 $S$ 上的凸函数，则对任意实数 $β_i≥0$，函数 $β_if_i$ 也是定义在 $S$ 上的凸函数；
    
    * 4. 若 $f$ 为定义在凸集 $S$ 上的凸函数，则对每一实数 $c$，水平集 $S_c=\{ x|x∈S，f(x)≤c \}$ 是凸集。

* 凹函数的性质和凸函数相反。

# 2. 常用向量距离准则
## 1. 欧式距离
* 欧式距离是我们在直角坐标系中最常用的距离量算方法，例如小时候学的“两点之间的最短距离是连接两点的直线距离”。这就是典型的欧式距离量算方法。通常这这个距离的获取是基于我们熟悉的“勾股定理”，解算三角形斜边得到的。

* 对于二维平面的两个点 $A(x_1,x_2)、B(y_1,y_2)$，其欧式距离为：
$$d = \sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$$

* 拓展到多维空间的两个点 $A(x_0,x_1,...,x_n)、B(y_0,y_1,...,y_n)$，其欧式距离为：
$$d_n = \sqrt{(x_0-y_0)^2+(x_1-y_1)^2+...+(x_n-y_n)^2}$$

## 2. 曼哈顿距离
* 曼哈顿距离是与欧式距离不同的一种丈量方法，两点之间的距离不再是直线距离，而是投影到坐标轴的长度之和。

![](https://upload-images.jianshu.io/upload_images/16911112-a646a2a3f0b12844.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中，图中绿色的线为欧式距离的丈量长度，红色的线即为曼哈顿距离长度，蓝色和黄色的线是这两点间曼哈顿距离的等价长度。

* 举个实际应用的例子，当我们从 A 街区去 B 街区时，中间隔着许多建筑物，我们无法走直线，这时候只能避开建筑物，这就是曼哈顿距离的实际应用。

## 3. 切比雪夫距离
* 数学上，切比雪夫距离是将2个点之间的距离定义为其各坐标数值差的最大值。
$$D_{chess} = max(|x_1-x_2|,|y_1-y_2|)$$

* 在棋盘上，使用的是离散的切比雪夫距离，以任意一个位置为准，和此点切比雪夫距离为 r 的所有位置也会形成一正方形，若以位置的中心量到其他位置的中心，此正方形的“边长”为 2r，正方形的边会有 2r+1 个方格，例如，和一位置切比雪夫距离为1的所有位置会形成一个 3×3 的正方形。也就是在下面3×3邻域内，中心网格的中心点到8个邻域网格中心点的距离相等。

![](https://upload-images.jianshu.io/upload_images/16911112-52474053ae4e5d4e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 4. 马氏距离
* 马氏距离(Mahalanobis Distance)是一种距离的度量，可以看作是欧氏距离的一种修正，修正了欧式距离中各个维度尺度不一致且相关的问题。单个数据点的马氏距离为：
$$D_M(x)=\sqrt{(x-\mu)^T\sum^-1(x-\mu)}$$

* 数据点 x,y 之间的马氏距离为：
$$D_M(x,y)=\sqrt{(x-y)^T\sum^-1(x-y)}$$
其中 Σ 是多维随机变量的协方差矩阵，μ 为样本均值，如果协方差矩阵是单位向量，也就是各维度独立同分布，马氏距离就变成了欧氏距离。

* 应用场景：身高和体重，这两个变量拥有不同的单位标准，也就是有不同的 scale。比如身高用毫米计算，而体重用千克计算，显然差10mm的身高与差10kg的体重是完全不同的。但在普通的欧氏距离中，这将会算作相同的差距。

* 详情请参考：[马氏距离](https://ph0en1xgseek.github.io/2018/04/18/Mahalanobis/)

## 5. 巴氏距离
* 在统计中，Bhattacharyya 距离测量两个离散或连续概率分布的相似性。它与衡量两个统计样品或种群之间的重叠量的 Bhattacharyya 系数密切相关。Bhattacharyya 距离和 Bhattacharyya 系数以20世纪30年代曾在印度统计研究所工作的一个统计学家A. Bhattacharya命名。同时，Bhattacharyya 系数可以被用来确定两个样本被认为相对接近的，它是用来测量中的类分类的可分离性。

* 巴氏距离的定义：对于离散概率分布 p 和 q 在同一域 X，它被定义为：
$$D_B(p,q)=-ln(BC(p,q))$$
其中，$BC(p,q)=\sum_{x\in X}\sqrt{p(x)q(x)}$ 是 Bhattacharyya 系数。

## 6. 汉明距离
* 汉明距离是使用在数据传输差错控制编码里面的，汉明距离是一个概念，它表示两个（相同长度）字对应位不同的数量，我们以 d（x,y） 表示两个字 x,y 之间的汉明距离。对两个字符串进行异或运算，并统计结果为1的个数，那么这个数就是汉明距离。

* 应用：
    * 1011101 与 1001001 之间的汉明距离是 2。
    * 2143896 与 2233796 之间的汉明距离是 3。
    * "toned" 与 "roses" 之间的汉明距离是 3。

## 7. 皮尔逊系数
### 7.1 相关系数
* 在具体阐述皮尔逊相关系数之前，有必要解释下什么是相关系数 ( Correlation coefficient )与相关距离(Correlation distance)。相关系数 ( Correlation coefficient )的定义是：
$$\rho_{xy}=\frac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}=\frac{E((X-EX)(Y-EY))}{\sqrt{D(X)}\sqrt{D(Y)}}$$
其中，E 为数学期望或均值，D 为方差，D 开根号为标准差，E(X-E(X))(Y-E(Y)) 称为随机变量 X 与 Y的协方差，记为 Cov(X,Y)，即 Cov(X,Y) = E(X-E(X))(Y-E(Y))，而两个变量之间的协方差和标准差的商则称为随机变量 X 与 Y 的相关系数，记为 $\rho_{xy}$。

* 相关系数衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明 X 与 Y 相关度越高。当 X 与 Y 线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）。具体的，如果有两个变量：X、Y，最终计算出的相关系数的含义可以有如下理解：

    * 当相关系数为0时，X 和 Y 两变量无关系。
    
    * 当 X 的值增大（减小），Y 值增大（减小），两个变量为正相关，相关系数在0.00与1.00之间。
    
    * 当 X 的值增大（减小），Y 值减小（增大），两个变量为负相关，相关系数在-1.00与0.00之间。

* 相关距离的定义是：
$$D_{xy}=1-\rho_{xy}$$

### 7.2 皮尔逊相关系数定义
* 在统计学中，皮尔逊积矩相关系数（英语：Pearson product-moment correlation coefficient，又称作 PPMCC或PCCs, 用r表示）用于度量两个变量 X 和 Y 之间的相关（线性相关），其值介于-1与1之间。通常情况下通过以下相关系数取值范围判断变量的相关强度：

    * 0.8-1.0：极强相关
    * 0.6-0.8：强相关
    * 0.4-0.6：中等程度相关
    * 0.2-0.4：弱相关
    * 0.0-0.2：极弱相关或无相关
    
这个相关系数也称作“皮尔森相关系数 r”。

* 皮尔逊系数的定义：两个变量之间的皮尔逊相关系数定义为两个变量之间的协方差和标准差的商：
$$\rho_{X,Y}=\frac{cov(X,Y)}{\sigma_X\sigma_Y}=\frac{E[(X-\mu_X)(Y-\mu_Y)]}{\sigma_X\sigma_Y}$$

* 基于样本对协方差和方差进行估计，可以得到样本标准差，一般表示成 r：
$$r=\frac{\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})}
{\sqrt{\sum_{i=1}^{n}(X_i-\bar{X})^2}\sqrt{\sum_{i=1}^{n}(Y_i-\bar{Y})^2}}$$

* 一种等价表达式的是表示成标准分的均值。基于(Xi, Yi)的样本点，样本皮尔逊系数是：
$$r=\frac{1}{n-1}\sum_{i=1}^{n}(\frac{X_i-\bar X}{s_X})(\frac{Y_i-\bar Y}{s_Y})$$

* 假设有两个变量X、Y，那么两变量间的皮尔逊相关系数可通过以下公式计算：
$$\rho_{X,Y}=\frac{cov(X,Y)}{\sigma_X\sigma_Y}=\frac{E[(X-\mu_X)(Y-\mu_Y)]}{\sigma_X\sigma_Y}$$

### 7.3 皮尔逊相关的约束条件
* 两个变量间有线性关系
* 变量是连续变量
* 变量均符合正态分布，且二元分布也符合正态分布
* 两变量独立

* 在实践统计中，一般只输出两个系数，一个是相关系数，也就是计算出来的相关系数大小，在-1到1之间;另一个是独立样本检验系数，用来检验样本一致性。

## 8. 信息熵
* 信息熵用于解决**信息的量化**问题，将原本模糊的信息概念进行计算得出精确的信息熵值，信息熵是**描述消息中**，**不确定性的值**。其公式如下所示：
$$H(x)=-\sum_{i=1}^{n}p(x_i)log(p(x_i))$$
其中 $p(x_i)$ 代表随机事件的概率。

* 下面正式引出信息熵：信息量度量的是一个具体事件发生了所带来的信息，而熵则是在结果出来之前对可能产生的信息量的期望，考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望。即：
$$H(x)=-sum(p(x)log_2p(x))\Longrightarrow H(x)=-\sum_{i=1}^{n}p(x_i)log(p(x_i))$$

* 信息熵还可以作为一个系统复杂程度的度量，如果系统越复杂，出现不同情况的种类越多，那么他的信息熵是比较大的。如果一个系统越简单，出现情况种类很少（极端情况为 1 种情况，那么对应概率为 1，那么对应的信息熵为 0），此时的信息熵较小。

## 9. 总结
简单说来，各种“距离”的应用场景简单概括为：

* 空间：欧氏距离；

* 路径：曼哈顿距离；

* 国际象棋国王：切比雪夫距离；

* 以上三种的统一形式:闵可夫斯基距离；

* 加权：标准化欧氏距离；

* 排除量纲和依存：马氏距离；

* 向量差距：夹角余弦；

* 编码差别：汉明距离；

* 集合近似度：杰卡德类似系数与距离；

* 相关：相关系数与相关距离。











