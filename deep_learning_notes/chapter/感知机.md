# 感知机
# 一、感知机原理
&#8195; 感知机是1957年，由 Rosenblatt 提出，是神经网络和支持向量机的基础。感知机是二分类的线性模型，其输入是实例的特征向量，输出的是事例的类别，分别是+1和-1，属于判别模型。

&#8195; 假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练数据集正实例点和负实例点完全正确分开的分离超平面。如果是非线性可分的数据，则最后无法获得超平面。如下图所示：

![](https://upload-images.jianshu.io/upload_images/16911112-770957de56af7050.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 1. 点到线的距离
&#8195; 公式中的直线方程为 Ax+By+C=0，点 P 的坐标为 $(x_0,y_0)$。
$$d=\frac{Ax_0+By_0+C}{\sqrt{A^2+B^2}}$$

## 2. 样本到超平面的距离
&#8195; 我们假设平面是 $h=w\cdot x+b$，其中 $w=(w_0,w_1,...,w_m),\;x=(x_0,x_1,...,x_m)$，样本点 $x'$ 到超平面的距离如下：
$$d=\frac{w\cdot x'}{||w||}$$

## 3. 超平面（Hyperplanes）
&#8195; 超平面是在空间 $R^d$ 中的一个子空间 $R^{d−1}$。在2维空间中的超平面是一条线，在3维空间中的超平面是一个平面。

## 4. 感知机的应用
1. 实现逻辑运算，包括逻辑和(AND)、逻辑或(OR)
2. 实现自我学习
3. 组成神经网络

---

# 二、感知机模型
&#8195; 感知机从输入空间到输出空间的模型如下：
$$f(x)=sign(w \cdot {x}+b)\\[2ex]
sign(x)= \begin{cases} -1，& {x<0}\\ 1，& {x\geq 0} \end{cases}$$

![](https://upload-images.jianshu.io/upload_images/16911112-107707bf458351d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 1. 感知机的损失函数
* 我们首先定义对于样本 $(x_i,y_i)$，如果 $\frac{w\cdot {x_i}+b}{||w||}>0$ 则记 $y_i=+1$，如果 $\frac{w\cdot {x_i}+b}{||w||}<0$ 则记 $y_i=-1$。

* 这样取y的值有一个好处，就是方便定义损失函数。因为正确分类的样本满足 $\frac{w\cdot {x_i}+b}{||w||}>0$，而错误分类的样本满足 $\frac{w\cdot {x_i}+b}{||w||}<0$。我们损失函数的优化目标，就是**期望使误分类的所有样本，到超平面的距离之和最小**。所以损失函数定义如下：
$$L(w,b)=-\frac{1}{||w||}\sum_{x_i\in{M}}y_i(w\cdot {x_i}+b)$$
其中 M 集合是误分类点的集合。

* 不考虑 $\frac{1}{||w||}$，就得到感知机模型的损失函数：
$$L(w,b)=-\frac{1}{||w||}\sum_{x_i\in{M}}y_i(w\cdot {x_i}+b)$$

## 2. 为什么可以不考虑 $\frac{1}{||w||}$
原因有二：
* 1. 感知机学习算法是误分类驱动的，这里需要注意的是所谓的“误分类驱动”指的是我们只需要判断 $-y_i(w⋅x_i+b)$ 的正负来判断分类的正确与否，而 $\frac{1}{||w||}$并不影响正负值的判断。所以 $\frac{1}{||w||}$对感知机学习算法的中间过程可以不考虑。

* 2. 感知机学习算法最终的终止条件是所有的输入都被正确分类，即不存在误分类的点。则此时损失函数为0. 对应于 $-\frac{1}{||w||}\sum_{i\in{M}}y_i(w\cdot {x_i}+b)$，即分子为0。则可以看出 $\frac{1}{||w||}$ 对最终结果也无影响。

综上所述，即使忽略 $\frac{1}{||w||}$，也不会对感知机学习算法的执行过程产生任何影响，反而还能简化运算，提高算法执行效率。

---

# 三、感知机算法
&#8195; 感知机学习算法是对上述损失函数进行极小化，求得 w 和 b。但是用普通的基于所有样本的梯度和的均值的批量梯度下降法（BGD）是行不通的，原因在于我们的损失函数里面有限定，**只有误分类的M集合里面的样本才能参与损失函数的优化**。所以我们不能用最普通的批量梯度下降，只能**采用随机梯度下降**（**SGD**）。目标函数如下：
$$L(w,b)=arg\min_{w,b}(-\sum\limits_{{{x}_{i}}\in{M}}{{{y}_{i}}(w\cdot {{x}_{i}}+b)})$$

## 1. 原始形式算法
* 输入：训练数据集 $T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}，y_i\in{\{-1,+1\}}$，学习率 $\eta(0<\eta<1)$；

* 输出：$w,b$；

* 感知机模型 $f(x)=sign(w⋅x+b)$。

* 算法流程：

    * 1. 赋初值 $w_0,b_0$；
    
    * 2. 选取数据点 $(x_i,y_i)$；
    
    * 3. 判断该数据点是否为当前模型的误分类点，即判断若 $y_i(w\cdot {x_i}+b)\leq0$ 则更新：
$$w={w+\eta{y_ix_i}}\\[2ex]
b={b+\eta{y_i}}$$

    * 4. 转到2，直到训练集中没有误分类点。

## 2. 对偶形式算法
* 由于 $w,b$ 的梯度更新公式：
$$w={w+\eta{y_ix_i}}\\[2ex]
b={b+\eta{y_i}}$$

* 我们的 $w,b$ 经过了 n 次修改后的，参数可以变化为下公式，其中 $α=ny$：
$$w=\sum_{x_i\in{M}}\eta{y_ix_i}=\sum_{i=1}^n\alpha_iy_ix_i\\[2ex]
b=\sum_{x_i\in{M}}\eta{y_i}=\sum_{i=1}^n\alpha_iy_i$$
这样我们就得出了感知机的对偶算法。

## 3. 算法流程：
* 输入：训练数据集 $T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}，y_i\in{\{-1,+1\}}$，学习率 $\eta(0<\eta<1)$；

* 输出：$\alpha,b$；

* 感知机模型 $f(x)=sign(\sum_{j=1}^n\alpha_jy_jx_j\cdot {x}+b)$，其中\alpha=(\alpha_1,\alpha_2,...,\alpha_n)^T。

    * 1. 赋初值 $\alpha_0,b_0$；
    
    * 2. 选取数据点 $(x_i,y_i)$；
    
    * 3. 判断该数据点是否为当前模型的误分类点，即判断：
$$y_i(\sum_{j=1}^n\alpha_jy_jx_j\cdot {x_i}+b)\leq0$$
则更新：
$$\alpha_i={\alpha_i+\eta},\quad b={b+\eta{y_i}}$$

    * 4. 转到2，直到训练集中没有误分类点。

* 为了减少计算量，我们可以预先计算式中的内积，得到 Gram 矩阵：
$$G=[x_i,x_j]_{N×N}$$

## 4. 原始形式和对偶形式的选择
* 在向量维数（特征数）过高时，计算内积非常耗时，应选择对偶形式算法加速。

* 在向量个数（样本数）过多时，每次计算累计和就没有必要，应选择原始算法。

## 5. 训练过程
* 线性可分的过程：

![](https://upload-images.jianshu.io/upload_images/16911112-9e65ceaa317b99b6.gif?imageMogr2/auto-orient/strip)

* 线性不可分的过程：

![](https://upload-images.jianshu.io/upload_images/16911112-736e762e2d7bc600.gif?imageMogr2/auto-orient/strip)

---

# 四、小结
&#8195; 　感知机算法是一个简单易懂的算法，自己编程实现也不太难。前面提到它是很多算法的鼻祖，比如**支持向量机算法**，**神经网络与深度学习**。因此虽然它现在已经不是一个在实践中广泛运用的算法，还是值得好好的去研究一下。感知机算法对偶形式为什么在实际运用中比原始形式快，也值得好好去体会。
